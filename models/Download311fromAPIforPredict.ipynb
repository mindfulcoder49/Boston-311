{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 01:14:12.759462: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-21 01:14:12.841056: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-21 01:14:12.841247: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-21 01:14:12.841427: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-21 01:14:12.859352: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-21 01:14:12.859967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-21 01:14:15.243805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "#import boston311 stuff\n",
    "from boston311 import Boston311LogReg, Boston311EventDecTree, Boston311SurvDecTree, Boston311KerasNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:272: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with different number of columns from File 0:  []\n",
      "Files with same number of columns as File 0:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Files with different column order from File 0:  []\n",
      "Files with same column order as File 0:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "data = Boston311LogReg(train_date_range={'start':'2010-12-31','end':'2023-12-31'}).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv as all_311_cases.csv\n",
    "data.to_csv('all_311_cases.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to a pkl file as all_311_cases.pkl\n",
    "data.to_pickle('all_311_cases.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load empty_response_ids.csv, remove duplicates, and save it again as empty_response_ids.csv\n",
    "data = pd.read_csv('empty_response_ids.csv')\n",
    "data = data.drop_duplicates(subset=['case_enquiry_id'])\n",
    "data.to_csv('empty_response_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file exists\n",
      "pkl file is newer than csv file\n",
      "empty_response_file exists\n",
      "Fetching data for service_request_id 101005131345\n",
      "Data written for service_request_id 101005131345\n",
      "Fetching data for service_request_id 101005131344\n",
      "Data written for service_request_id 101005131344\n",
      "Fetching data for service_request_id 101005131343\n",
      "Data written for service_request_id 101005131343\n",
      "Fetching data for service_request_id 101005131342\n",
      "Data written for service_request_id 101005131342\n",
      "Fetching data for service_request_id 101005131341\n",
      "Data written for service_request_id 101005131341\n",
      "Fetching data for service_request_id 101005131340\n",
      "Data written for service_request_id 101005131340\n",
      "Fetching data for service_request_id 101005131339\n",
      "Data written for service_request_id 101005131339\n",
      "Fetching data for service_request_id 101005131338\n",
      "Data written for service_request_id 101005131338\n",
      "Fetching data for service_request_id 101005131337\n",
      "Data written for service_request_id 101005131337\n",
      "Fetching data for service_request_id 101005131336\n",
      "Data written for service_request_id 101005131336\n",
      "Fetching data for service_request_id 101005131335\n",
      "Empty response for service_request_id 101005131335\n",
      "Fetching data for service_request_id 101005131332\n",
      "Empty response for service_request_id 101005131332\n",
      "Fetching data for service_request_id 101005131330\n",
      "Empty response for service_request_id 101005131330\n",
      "Fetching data for service_request_id 101005131328\n",
      "Empty response for service_request_id 101005131328\n",
      "Fetching data for service_request_id 101005131318\n",
      "Empty response for service_request_id 101005131318\n",
      "Fetching data for service_request_id 101005131314\n",
      "Data written for service_request_id 101005131314\n",
      "Fetching data for service_request_id 101005131300\n",
      "Data written for service_request_id 101005131300\n",
      "Fetching data for service_request_id 101005131299\n",
      "Data written for service_request_id 101005131299\n",
      "Fetching data for service_request_id 101005131292\n",
      "Data written for service_request_id 101005131292\n",
      "Fetching data for service_request_id 101005131237\n",
      "Data written for service_request_id 101005131237\n",
      "Fetching data for service_request_id 101005131233\n",
      "Data written for service_request_id 101005131233\n",
      "Fetching data for service_request_id 101005131227\n",
      "Data written for service_request_id 101005131227\n",
      "Fetching data for service_request_id 101005131208\n",
      "Data written for service_request_id 101005131208\n",
      "Fetching data for service_request_id 101005131184\n",
      "Data written for service_request_id 101005131184\n",
      "Fetching data for service_request_id 101005131172\n",
      "Data written for service_request_id 101005131172\n",
      "Fetching data for service_request_id 101005131171\n",
      "Data written for service_request_id 101005131171\n",
      "Fetching data for service_request_id 101005131170\n",
      "Empty response for service_request_id 101005131170\n",
      "Fetching data for service_request_id 101005131168\n",
      "Empty response for service_request_id 101005131168\n",
      "Fetching data for service_request_id 101005131166\n",
      "Data written for service_request_id 101005131166\n",
      "Fetching data for service_request_id 101005131165\n",
      "Empty response for service_request_id 101005131165\n",
      "Fetching data for service_request_id 101005131161\n",
      "Data written for service_request_id 101005131161\n",
      "Fetching data for service_request_id 101005131160\n",
      "Data written for service_request_id 101005131160\n",
      "Fetching data for service_request_id 101005131159\n",
      "Data written for service_request_id 101005131159\n",
      "Fetching data for service_request_id 101005131158\n",
      "Data written for service_request_id 101005131158\n",
      "Fetching data for service_request_id 101005131157\n",
      "Data written for service_request_id 101005131157\n",
      "Fetching data for service_request_id 101005131155\n",
      "Data written for service_request_id 101005131155\n",
      "Fetching data for service_request_id 101005131143\n",
      "Data written for service_request_id 101005131143\n",
      "Fetching data for service_request_id 101005131142\n",
      "Data written for service_request_id 101005131142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbriarmossdesktop/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWaiting \u001b[39m\u001b[39m{\u001b[39;00mrate_limit_delay\u001b[39m}\u001b[39;00m\u001b[39m seconds before retrying\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbriarmossdesktop/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m     \u001b[39m# Rate limiting\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbriarmossdesktop/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(rate_limit_delay)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbriarmossdesktop/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# Close CSV file\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbriarmossdesktop/home/briarmoss/Documents/Boston_311/models/Download311fromAPIforPredict.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m csv_file\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load your CSVs into pandas DataFrames\n",
    "\n",
    "case_enquiry_id_file = \"all_311_cases.csv\"\n",
    "service_request_id_file = \"all_311_cases_api.csv\"\n",
    "case_enquiry_id_pkl = \"all_311_cases.pkl\"\n",
    "empty_response_file = \"empty_response_ids.csv\"\n",
    "\n",
    "#check if pkl file exists\n",
    "if os.path.exists(case_enquiry_id_pkl):\n",
    "    print(\"pkl file exists\")\n",
    "    #check if pkl file modified date is older than csv file\n",
    "    if os.path.getmtime(case_enquiry_id_pkl) < os.path.getmtime(case_enquiry_id_file):\n",
    "        print(\"pkl file is older than csv file\")\n",
    "        #delete pkl file\n",
    "        os.remove(case_enquiry_id_pkl)\n",
    "        print(\"pkl file deleted\")\n",
    "        #read csv file\n",
    "        df1 = pd.read_csv(case_enquiry_id_file)\n",
    "        #save csv file as pkl file\n",
    "        df1.to_pickle(case_enquiry_id_pkl)\n",
    "    else:\n",
    "        print(\"pkl file is newer than csv file\")\n",
    "        df1 = pd.read_pickle(case_enquiry_id_pkl)\n",
    "else:\n",
    "    print(\"pkl file does not exist\")\n",
    "    df1 = pd.read_csv(case_enquiry_id_file)\n",
    "    df1.to_pickle(case_enquiry_id_pkl)\n",
    "\n",
    "\n",
    "\n",
    "#file with service_request_id\n",
    "df2 = pd.read_csv(service_request_id_file)\n",
    "\n",
    "#check if empty_response_file exists\n",
    "if os.path.exists(empty_response_file):\n",
    "    print(\"empty_response_file exists\")\n",
    "    df3 = pd.read_csv(empty_response_file)\n",
    "\n",
    "\n",
    "# Identify missing service_request_ids\n",
    "missing_ids = set(df1['case_enquiry_id']) - set(df2['service_request_id'])\n",
    "\n",
    "#also subtract the empty_response_file\n",
    "if df3 is not None:\n",
    "    missing_ids = missing_ids - set(df3['case_enquiry_id'])\n",
    "\n",
    "\n",
    "\n",
    "missing_records = df1[df1['case_enquiry_id'].isin(missing_ids)]\n",
    "missing_records = missing_records.sort_values(by='case_enquiry_id', ascending=False)\n",
    "\n",
    "#initialize CSV file for empty response IDs\n",
    "#if the file exists, append to it\n",
    "if os.path.exists(empty_response_file):\n",
    "    empty_response_csv_file = open(empty_response_file, 'a', newline='', encoding='utf-8')\n",
    "    empty_response_csv_writer = csv.writer(empty_response_csv_file)\n",
    "else:\n",
    "    empty_response_csv_file = open(empty_response_file, 'w', newline='', encoding='utf-8')\n",
    "    empty_response_csv_writer = csv.writer(empty_response_csv_file)\n",
    "    empty_response_csv_writer.writerow(['case_enquiry_id'])\n",
    "\n",
    "# Initialize CSV file\n",
    "csv_file_path = service_request_id_file\n",
    "csv_file = open(csv_file_path, 'a', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# Rate limit delay\n",
    "rate_limit_delay = 6  # 6 seconds to stay within 10 requests per minute\n",
    "max_exponential_backoff = 3600 \n",
    "\n",
    "for service_request_id in missing_records['case_enquiry_id']:\n",
    "    url = f\"https://311.boston.gov/open311/v2/requests.json?service_request_id={service_request_id}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        print(f\"Fetching data for service_request_id {service_request_id}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = json.loads(response.text)\n",
    "                if not data:  # if data is empty\n",
    "                    empty_response_csv_writer.writerow([service_request_id])\n",
    "                    empty_response_csv_file.flush()\n",
    "                    print(f\"Empty response for service_request_id {service_request_id}\")\n",
    "                else:\n",
    "                    for record in data:\n",
    "                        #print all the data\n",
    "                        #print(record.get('service_request_id'), record.get('status'), record.get('service_name'), record.get('service_code'), record.get('description'), record.get('requested_datetime'), record.get('updated_datetime'), record.get('address'), record.get('lat'), record.get('long'), record.get('token'))\n",
    "                        csv_writer.writerow([\n",
    "                            record.get('service_request_id'),\n",
    "                            record.get('status'),\n",
    "                            record.get('service_name'),\n",
    "                            record.get('service_code'),\n",
    "                            record.get('description'),\n",
    "                            record.get('requested_datetime'),\n",
    "                            record.get('updated_datetime'),\n",
    "                            record.get('address'),\n",
    "                            record.get('lat'),\n",
    "                            record.get('long'),\n",
    "                            record.get('token')\n",
    "                        ])\n",
    "                        csv_file.flush()\n",
    "                        print(f\"Data written for service_request_id {service_request_id}\")\n",
    "                    rate_limit_delay = 6\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to decode JSON for service_request_id {service_request_id}\")\n",
    "                # Double the delay time after failure\n",
    "                rate_limit_delay = min(rate_limit_delay * 2, max_exponential_backoff)\n",
    "                print(f\"Waiting {rate_limit_delay} seconds before retrying\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for service_request_id {service_request_id}\")\n",
    "            # Double the delay time after failure\n",
    "            rate_limit_delay = min(rate_limit_delay * 2, max_exponential_backoff)\n",
    "            print(f\"Waiting {rate_limit_delay} seconds before retrying\")   \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Failed to connect to {url}\")\n",
    "        # Double the delay time after failure\n",
    "        rate_limit_delay = min(rate_limit_delay * 2, max_exponential_backoff)\n",
    "        print(f\"Waiting {rate_limit_delay} seconds before retrying\")\n",
    "\n",
    "    # Rate limiting\n",
    "    time.sleep(rate_limit_delay)\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
