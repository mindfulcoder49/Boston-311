{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Processing /home/briarmoss/Documents/Boston_311\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: boston311\n",
            "  Building wheel for boston311 (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for boston311: filename=boston311-0.2.0-py3-none-any.whl size=23980 sha256=89136a3c02373d9e07b9ff81c09e38aa4aa40f764a88ad70a7b0018c4fbfe49d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i6pcy3a_/wheels/3d/69/ee/0a6ac96b9c09c948fc0e74f2724a9703aa39749a41fa757c9e\n",
            "Successfully built boston311\n",
            "Installing collected packages: boston311\n",
            "  Attempting uninstall: boston311\n",
            "    Found existing installation: boston311 0.2.0\n",
            "    Uninstalling boston311-0.2.0:\n",
            "      Successfully uninstalled boston311-0.2.0\n",
            "Successfully installed boston311-0.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install  ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-31 23:10:50.946645: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-31 23:10:51.196135: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-31 23:10:51.197699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-31 23:10:51.878190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/tmp/ipykernel_505058/1626478388.py:5: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperParameters\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from kerastuner import HyperParameters\n",
        "from boston311 import Boston311LogReg, Boston311KerasNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUMCPAELR9h7",
        "outputId": "05fcba64-1b74-4fb7-a52c-d3f8a499bcc6"
      },
      "outputs": [],
      "source": [
        "today_datestring, tomorrow_datestring, thirty_days_ago_datestring = Boston311LogReg().get_datestrings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set model folder constant\n",
        "MODEL_FOLDER = './daily_models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXTRA_mydata_FILE = './cls_and_pooled_embeddings_with_three_cols.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl_load_data = Boston311LogReg().pkl_load_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "KerasNN_model = Boston311KerasNN(train_date_range={'start':'2023-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['queue', 'subject', 'reason', 'department'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':300,\n",
        "                                      'survivalTimeFill':tomorrow_datestring})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get current datetime in Boston timezone as string\n",
        "my_datetime_str = Boston311LogReg().get_current_datetime_str()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "case_data_file = 'case_data.pkl'\n",
        "case_data_csv = 'all_311_cases.csv'\n",
        "data = pkl_load_data(case_data_csv, case_data_file)\n",
        "mydata = KerasNN_model.load_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = KerasNN_model.enhance_data(mydata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = KerasNN_model.apply_scenario(mydata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = KerasNN_model.clean_data(mydata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "import pickle\n",
        "\n",
        "pickle_file = 'dataframe.pkl'\n",
        "\n",
        "df = None\n",
        "\n",
        "df = pkl_load_data(EXTRA_mydata_FILE, pickle_file)\n",
        "\n",
        "# if df has a column service_request_id, do the following\n",
        "if 'service_request_id' in df.columns:\n",
        "    df.rename(columns={'service_request_id':'case_enquiry_id'}, inplace=True)\n",
        "    for col in ['desc_cls_embedding', 'desc_pooled_embedding', 'name_cls_embedding', 'name_pooled_embedding', 'code_cls_embedding', 'code_pooled_embedding']:\n",
        "        df[col] = df[col].apply(literal_eval).apply(np.array)\n",
        "\n",
        "    pickle.dump(df, open(pickle_file, \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_and_replace_columns(df, column_names):\n",
        "    new_dfs = []\n",
        "    \n",
        "    # Flatten and remove original columns\n",
        "    for col_name in column_names:\n",
        "        flattened = np.stack(df[col_name].to_numpy())\n",
        "        new_df = pd.DataFrame(flattened, columns=[f'{col_name}_{i}' for i in range(flattened.shape[1])])\n",
        "        new_dfs.append(new_df)\n",
        "        df.drop([col_name], axis=1, inplace=True)\n",
        "        \n",
        "    # Concatenate new columns to the original DataFrame\n",
        "    df = pd.concat([df] + new_dfs, axis=1)\n",
        "    return df\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "column_names = ['desc_cls_embedding', 'desc_pooled_embedding', 'name_cls_embedding', 'name_pooled_embedding', 'code_cls_embedding', 'code_pooled_embedding']\n",
        "df = flatten_and_replace_columns(df, column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['case_enquiry_id'] = df['case_enquiry_id'].astype(str)\n",
        "is_numeric = df['case_enquiry_id'].str.isnumeric()\n",
        "df = df[is_numeric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['case_enquiry_id'] = df['case_enquiry_id'].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset=['case_enquiry_id']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = mydata.drop_duplicates(subset=['case_enquiry_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#join them so we are left only with records that have mydata in both files\n",
        "new_mydata = mydata.merge(df, on='case_enquiry_id', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_mydata = new_mydata.sort_values(by='case_enquiry_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "hour_interval = 72\n",
        "max_days = 180\n",
        "#bin_edges = KerasNN_model.generate_time_bins_statistics(df, num_intervals=60)\n",
        "bin_edges= KerasNN_model.generate_time_bins_fixed_interval(hour_interval, max_days)\n",
        "bin_labels= KerasNN_model.generate_bin_labels(bin_edges)\n",
        "bin_number = len(bin_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df, y = KerasNN_model.split_data(new_mydata, bin_edges=bin_edges, bin_labels=bin_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cast all columns that are type bool to float\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'bool':\n",
        "        df[col] = df[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#best_model, best_hyperparameters = KerasNN_model.tune_model(df, y, '/home/briarmoss/Documents/Boston_311/models/tuning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set constants\n",
        "start_nodes = 1024  \n",
        "end_nodes = 128\n",
        "#l2_0 = 0.00001\n",
        "#learning_rate = 7.5842e-05\n",
        "l2_0 = 0.001\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "hp = HyperParameters()\n",
        "hp.Fixed('start_nodes', start_nodes)\n",
        "hp.Fixed('end_nodes', end_nodes)\n",
        "hp.Fixed('l2_0', l2_0)\n",
        "hp.Fixed('learning_rate', learning_rate)\n",
        "hp.Fixed('final_layer', bin_number)\n",
        "hp.Fixed('final_activation', 'softmax')\n",
        "KerasNN_model.best_hyperparameters = hp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training at 2023-10-31 23:11:12.240150\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1024)              1033216   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 61)                7869      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1730109 (6.60 MB)\n",
            "Trainable params: 1730109 (6.60 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'> (94469, 61)\n",
            "<class 'pandas.core.frame.DataFrame'> (23618, 61)\n",
            "run fit\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-31 23:11:12.937621: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 761798016 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "739/739 [==============================] - 10s 13ms/step - loss: 2.6450 - accuracy: 0.7086 - top_k_categorical_accuracy: 0.7924 - val_loss: 2.0150 - val_accuracy: 0.7277 - val_top_k_categorical_accuracy: 0.8174\n",
            "Epoch 2/2\n",
            "739/739 [==============================] - 9s 13ms/step - loss: 1.9526 - accuracy: 0.7148 - top_k_categorical_accuracy: 0.8009 - val_loss: 1.6981 - val_accuracy: 0.7336 - val_top_k_categorical_accuracy: 0.8204\n",
            "739/739 [==============================] - 1s 1ms/step - loss: 1.8118 - accuracy: 0.7155 - top_k_categorical_accuracy: 0.8040\n",
            "Testing accuracy: 0.7155135869979858 \n",
            "Top-2 accuracy: 0.8039630651473999 \n",
            "Test loss: 1.8117965459823608\n",
            "Ending Training at 2023-10-31 23:11:34.101699\n",
            "Training took 0:00:21.861549\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#parse CLS embedding column as array\n",
        "test_acc = KerasNN_model.train_model( df, y , epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/briarmoss/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def save_model_to_dir(model, folder_name):\n",
        "    dir_path = os.path.join(MODEL_FOLDER, folder_name)\n",
        "    \n",
        "    if not os.path.exists(dir_path):\n",
        "        os.mkdir(dir_path)\n",
        "    \n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    model_name = timestamp + \"_\" + model.model_type\n",
        "    properties_name = model_name\n",
        "    \n",
        "    model.save(dir_path, model_name, properties_name)\n",
        "\n",
        "\n",
        "save_model_to_dir(KerasNN_model, KerasNN_model.model_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndata = KerasNN_model.load_data( 'predict' )\\ndata = KerasNN_model.enhance_data( data, 'predict')\\nclean_data = KerasNN_model.clean_data_for_prediction( data )\\n\\nX_predict, y_predict = KerasNN_model.split_data( clean_data )\\ny_predict = KerasNN_model.model.predict(X_predict)\\ndata['survival_prediction'] = y_predict\\nreturn data\\n\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "data = KerasNN_model.load_data( 'predict' )\n",
        "data = KerasNN_model.enhance_data( data, 'predict')\n",
        "clean_data = KerasNN_model.clean_data_for_prediction( data )\n",
        "\n",
        "X_predict, y_predict = KerasNN_model.split_data( clean_data )\n",
        "y_predict = KerasNN_model.model.predict(X_predict)\n",
        "data['survival_prediction'] = y_predict\n",
        "return data\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
