{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1BXqsllgueC"
      },
      "source": [
        "#Boston 311 Tutorial\n",
        "\n",
        "This notebook will run you through the basic usage of this package to train 3 models on the Boston 311 mydata and use them to predict the outcome of cases from the last 30 days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Processing /home/briarmoss/Documents/Boston_311\n",
            "  Installing build dependencies ... \u001b[?25l"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: boston311\n",
            "  Building wheel for boston311 (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for boston311: filename=boston311-0.1.0-py3-none-any.whl size=18968 sha256=490648daa4c548a90a18e3e8f5f3f300f87db0f48ace65acdc1256cbf9b67597\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o7868fja/wheels/3d/69/ee/0a6ac96b9c09c948fc0e74f2724a9703aa39749a41fa757c9e\n",
            "Successfully built boston311\n",
            "Installing collected packages: boston311\n",
            "  Attempting uninstall: boston311\n",
            "    Found existing installation: boston311 0.1.0\n",
            "    Uninstalling boston311-0.1.0:\n",
            "      Successfully uninstalled boston311-0.1.0\n",
            "Successfully installed boston311-0.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install  ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP7Hzen_iEAl"
      },
      "source": [
        "##Import the Boston311Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LdhJESm7eWaY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 01:06:40.346847: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-21 01:06:40.382022: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-21 01:06:40.382057: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-21 01:06:40.382095: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-21 01:06:40.389223: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-21 01:06:40.389836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-21 01:06:41.058593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "from boston311 import Boston311LogReg, Boston311EventDecTree, Boston311SurvDecTree, Boston311KerasNLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq06XnVjPxvg"
      },
      "source": [
        "## Get latest file URLS and Current Date Ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_2FYkUb2P1LR"
      },
      "outputs": [],
      "source": [
        "latest_URLS = Boston311LogReg.get311URLs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpg9Czm3QDpu",
        "outputId": "5e2b399b-0999-45d4-f9f9-d80ec19941ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'2023': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/e6013a93-1321-4f2a-bf91-8d8a02f1e62f/download/tmpktyorzsa.csv', '2022': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/81a7b022-f8fc-4da5-80e4-b160058ca207/download/tmpfm8veglw.csv', '2021': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/f53ebccd-bc61-49f9-83db-625f209c95f5/download/tmp88p9g82n.csv', '2020': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/6ff6a6fd-3141-4440-a880-6f60a37fe789/download/tmpcv_10m2s.csv', '2019': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/ea2e4696-4a2d-429c-9807-d02eb92e0222/download/tmpcje3ep_w.csv', '2018': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/2be28d90-3a90-4af1-a3f6-f28c1e25880a/download/tmp7602cia8.csv', '2017': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/30022137-709d-465e-baae-ca155b51927d/download/tmpzccn8u4q.csv', '2016': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/b7ea6b1b-3ca4-4c5b-9713-6dc1db52379a/download/tmpzxzxeqfb.csv', '2015': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/c9509ab4-6f6d-4b97-979a-0cf2a10c922b/download/tmphrybkxuh.csv', '2014': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/bdae89c8-d4ce-40e9-a6e1-a5203953a2e0/download/tmp8afxvko_.csv', '2013': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/407c5cd0-f764-4a41-adf8-054ff535049e/download/tmpyzk_wmya.csv', '2012': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/382e10d9-1864-40ba-bef6-4eea3c75463c/download/tmpeyvgdt5u.csv', '2011': 'https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/94b499d9-712a-4d2a-b790-7ceec5c9c4b1/download/tmp_9ogynu0.csv'}\n"
          ]
        }
      ],
      "source": [
        "print(latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUMCPAELR9h7",
        "outputId": "05fcba64-1b74-4fb7-a52c-d3f8a499bcc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-21 2023-09-21 2023-10-22\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "now = datetime.now()\n",
        "thirty_days = timedelta(days=30)\n",
        "thirty_days_ago = now - thirty_days\n",
        "today_datestring = now.strftime(\"%Y-%m-%d\")\n",
        "thirty_days_ago_datestring = thirty_days_ago.strftime(\"%Y-%m-%d\")\n",
        "tomorrow_datestring = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "print(today_datestring, thirty_days_ago_datestring, tomorrow_datestring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set model folder constant\n",
        "MODEL_FOLDER = './daily_models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set path to mydata\n",
        "EXTRA_mydata_FILE = './cls_and_pooled_embeddings_with_three_cols.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define a function that takes a path to a csv file and a pkl file and checks if the csv file is newer than the pkl file, and if so, loads the csv file into a dataframe and saves it as a pkl file, else loads the pkl file into a dataframe\n",
        "def pkl_load_data(csv_path, pkl_path):\n",
        "    if os.path.exists(pkl_path):\n",
        "        pkl_time = os.path.getmtime(pkl_path)\n",
        "        csv_time = os.path.getmtime(csv_path)\n",
        "        if csv_time > pkl_time:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            df.to_pickle(pkl_path)\n",
        "        else:\n",
        "            df = pd.read_pickle(pkl_path)\n",
        "    else:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.to_pickle(pkl_path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN_UiAIvb9MT"
      },
      "source": [
        "##Define several models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nmi4jJgDF4Fv"
      },
      "outputs": [],
      "source": [
        "linear_tree_model = Boston311SurvDecTree(train_date_range={'start':'2022-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['type','queue'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':0,\n",
        "                                      'survivalTimeFill':tomorrow_datestring},\n",
        "                            files_dict=latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GGSlYgH6s54c"
      },
      "outputs": [],
      "source": [
        "logistic_model = Boston311LogReg(train_date_range={'start':'2022-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['type', 'queue'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':0},\n",
        "                            files_dict=latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "oldlogistic_model = Boston311LogReg(train_date_range={'start':'2022-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['type', 'queue'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':0},\n",
        "                            files_dict=latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_Ddtw6t8s5rj"
      },
      "outputs": [],
      "source": [
        "logistic_tree_model = Boston311EventDecTree(train_date_range={'start':'2022-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['type', 'queue'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':0},\n",
        "                            files_dict=latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "kerasNLP_model = Boston311KerasNLP(train_date_range={'start':'2010-01-01','end':thirty_days_ago_datestring},\n",
        "                            predict_date_range={'start':thirty_days_ago_datestring,'end':today_datestring},\n",
        "                            feature_columns=['queue', 'subject', 'reason', 'department'],\n",
        "                            scenario={'dropColumnValues': {'source':['City Worker App', 'Employee Generated']},\n",
        "                                      'survivalTimeMin':0,\n",
        "                                      'survivalTimeFill':tomorrow_datestring},\n",
        "                            files_dict=latest_URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get current datetime in Boston timezone as string\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "boston = timezone('US/Eastern')\n",
        "now = datetime.now(boston)\n",
        "today_datestring = now.strftime(\"%Y-%m-%d\")\n",
        "#get time in Boston timezone as string for a filename\n",
        "now = datetime.now(boston)\n",
        "time_string = now.strftime(\"%H-%M-%S\")\n",
        "#define datetime string\n",
        "my_datetime = today_datestring + '_' + time_string "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = None\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "case_data_file = 'case_data.pkl'\n",
        "case_data_csv = 'all_311_cases.csv'\n",
        "mydata = None\n",
        "\n",
        "X = None\n",
        "\n",
        "\n",
        "data = pkl_load_data(case_data_csv, case_data_file)\n",
        "mydata = kerasNLP_model.load_data(data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          101000295613\n",
              "1          101000295614\n",
              "2          101000295615\n",
              "3          101000295616\n",
              "4          101000295617\n",
              "               ...     \n",
              "2700977    101005074104\n",
              "2700978    101005074105\n",
              "2700979    101005074106\n",
              "2700980    101005074108\n",
              "2700981    101005074110\n",
              "Name: case_enquiry_id, Length: 2700982, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydata['case_enquiry_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:90: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['closed_dt'] = pd.to_datetime(data['closed_dt'])\n",
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:91: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['open_dt'] = pd.to_datetime(data['open_dt'])\n",
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:92: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['survival_time'] = data['closed_dt'] - data['open_dt']\n",
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['event'] = data['closed_dt'].notnull().astype(int)\n",
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['ward_number'] = data['ward'].str.extract(r'0*(\\d+)')\n",
            "/home/briarmoss/.local/lib/python3.10/site-packages/boston311/Boston311Model.py:97: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['survival_time_hours'] = np.nan\n"
          ]
        }
      ],
      "source": [
        "mydata = kerasNLP_model.enhance_data(mydata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = kerasNLP_model.apply_scenario(mydata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mydata = kerasNLP_model.clean_data(mydata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1          101000295614\n",
            "2          101000295615\n",
            "3          101000295616\n",
            "4          101000295617\n",
            "5          101000295618\n",
            "               ...     \n",
            "2700977    101005074104\n",
            "2700978    101005074105\n",
            "2700979    101005074106\n",
            "2700980    101005074108\n",
            "2700981    101005074110\n",
            "Name: case_enquiry_id, Length: 2267839, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(mydata['case_enquiry_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import pickle\n",
        "\n",
        "pickle_file = 'dataframe.pkl'\n",
        "\n",
        "X = None\n",
        "\n",
        "X = pkl_load_data(EXTRA_mydata_FILE, pickle_file)\n",
        "\n",
        "# if X has a column service_request_id, do the following\n",
        "if 'service_request_id' in X.columns:\n",
        "    X.rename(columns={'service_request_id':'case_enquiry_id'}, inplace=True)\n",
        "    #X['cls_embedding'] = X['cls_embedding'].apply(literal_eval).apply(np.array)\n",
        "    #X['pooled_embedding'] = X['pooled_embedding'].apply(literal_eval).apply(np.array)\n",
        "    #new code similar two above two lines but using the six columns of embeddings: desc_cls_embedding, desc_pooled_embedding, name_cls_embedding, name_pooled_embedding, code_cls_embedding, code_pooled_embedding\n",
        "    for col in ['desc_cls_embedding', 'desc_pooled_embedding', 'name_cls_embedding', 'name_pooled_embedding', 'code_cls_embedding', 'code_pooled_embedding']:\n",
        "        X[col] = X[col].apply(literal_eval).apply(np.array)\n",
        "\n",
        "    pickle.dump(X, open(pickle_file, \"wb\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 279547 entries, 0 to 279546\n",
            "Data columns (total 7 columns):\n",
            " #   Column                 Non-Null Count   Dtype \n",
            "---  ------                 --------------   ----- \n",
            " 0   case_enquiry_id        279547 non-null  int64 \n",
            " 1   desc_cls_embedding     279547 non-null  object\n",
            " 2   desc_pooled_embedding  279547 non-null  object\n",
            " 3   name_cls_embedding     279547 non-null  object\n",
            " 4   name_pooled_embedding  279547 non-null  object\n",
            " 5   code_cls_embedding     279547 non-null  object\n",
            " 6   code_pooled_embedding  279547 non-null  object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 14.9+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#print information about X2022\n",
        "print(X.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#concatenate the two dataframes and reindex\n",
        "df = X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(279547, 7)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assuming df is your DataFrame and it has columns 'desc_cls_embedding', 'desc_pooled_embedding', 'name_cls_embedding', 'name_pooled_embedding', 'code_cls_embedding', 'code_pooled_embedding'\n",
        "desc_cls_embedding_flattened = np.stack(df['desc_cls_embedding'].to_numpy())\n",
        "desc_pooled_embedding_flattened = np.stack(df['desc_pooled_embedding'].to_numpy())\n",
        "#do the same for the rest\n",
        "name_cls_embedding_flattened = np.stack(df['name_cls_embedding'].to_numpy())\n",
        "name_pooled_embedding_flattened = np.stack(df['name_pooled_embedding'].to_numpy())\n",
        "code_cls_embedding_flattened = np.stack(df['code_cls_embedding'].to_numpy())\n",
        "code_pooled_embedding_flattened = np.stack(df['code_pooled_embedding'].to_numpy())\n",
        "\n",
        "\n",
        "# Remove the old columns\n",
        "df.drop(['desc_cls_embedding', 'desc_pooled_embedding'], axis=1, inplace=True)\n",
        "#do the same for the rest\n",
        "df.drop(['name_cls_embedding', 'name_pooled_embedding'], axis=1, inplace=True)\n",
        "df.drop(['code_cls_embedding', 'code_pooled_embedding'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add the new flattened columns\n",
        "df_desc_cls = pd.DataFrame(desc_cls_embedding_flattened, columns=[f'desc_cls_{i}' for i in range(desc_cls_embedding_flattened.shape[1])])\n",
        "\n",
        "df_desc_pooled = pd.DataFrame(desc_pooled_embedding_flattened, columns=[f'desc_pooled_{i}' for i in range(desc_pooled_embedding_flattened.shape[1])])\n",
        "#do the same for the rest\n",
        "df_name_cls = pd.DataFrame(name_cls_embedding_flattened, columns=[f'name_cls_{i}' for i in range(name_cls_embedding_flattened.shape[1])])\n",
        "df_name_pooled = pd.DataFrame(name_pooled_embedding_flattened, columns=[f'name_pooled_{i}' for i in range(name_pooled_embedding_flattened.shape[1])])\n",
        "df_code_cls = pd.DataFrame(code_cls_embedding_flattened, columns=[f'code_cls_{i}' for i in range(code_cls_embedding_flattened.shape[1])])\n",
        "df_code_pooled = pd.DataFrame(code_pooled_embedding_flattened, columns=[f'code_pooled_{i}' for i in range(code_pooled_embedding_flattened.shape[1])])\n",
        "\n",
        "\n",
        "\n",
        "df = pd.concat([df, df_desc_cls, df_desc_pooled, df_name_cls, df_name_pooled, df_code_cls, df_code_pooled], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['case_enquiry_id'] = df['case_enquiry_id'].astype(str)\n",
        "is_numeric = df['case_enquiry_id'].str.isnumeric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[is_numeric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['case_enquiry_id'] = df['case_enquiry_id'].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(279547, 769)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset=['case_enquiry_id']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(279547, 769)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_enquiry_id</th>\n",
              "      <th>desc_cls_0</th>\n",
              "      <th>desc_cls_1</th>\n",
              "      <th>desc_cls_2</th>\n",
              "      <th>desc_cls_3</th>\n",
              "      <th>desc_cls_4</th>\n",
              "      <th>desc_cls_5</th>\n",
              "      <th>desc_cls_6</th>\n",
              "      <th>desc_cls_7</th>\n",
              "      <th>desc_cls_8</th>\n",
              "      <th>...</th>\n",
              "      <th>code_pooled_118</th>\n",
              "      <th>code_pooled_119</th>\n",
              "      <th>code_pooled_120</th>\n",
              "      <th>code_pooled_121</th>\n",
              "      <th>code_pooled_122</th>\n",
              "      <th>code_pooled_123</th>\n",
              "      <th>code_pooled_124</th>\n",
              "      <th>code_pooled_125</th>\n",
              "      <th>code_pooled_126</th>\n",
              "      <th>code_pooled_127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101004113559</td>\n",
              "      <td>-1.100319</td>\n",
              "      <td>0.180848</td>\n",
              "      <td>-3.067521</td>\n",
              "      <td>-2.449431</td>\n",
              "      <td>0.048062</td>\n",
              "      <td>0.750774</td>\n",
              "      <td>-1.156688</td>\n",
              "      <td>1.701071</td>\n",
              "      <td>-1.121157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121683</td>\n",
              "      <td>-0.998929</td>\n",
              "      <td>0.036228</td>\n",
              "      <td>-0.999963</td>\n",
              "      <td>-0.742293</td>\n",
              "      <td>0.969737</td>\n",
              "      <td>-0.998525</td>\n",
              "      <td>0.986848</td>\n",
              "      <td>0.982758</td>\n",
              "      <td>0.931549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>101004113295</td>\n",
              "      <td>-0.136960</td>\n",
              "      <td>0.691521</td>\n",
              "      <td>-3.540846</td>\n",
              "      <td>-1.352687</td>\n",
              "      <td>1.299200</td>\n",
              "      <td>-0.141181</td>\n",
              "      <td>0.158119</td>\n",
              "      <td>2.410162</td>\n",
              "      <td>0.071449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086003</td>\n",
              "      <td>-0.998614</td>\n",
              "      <td>0.014076</td>\n",
              "      <td>-0.999995</td>\n",
              "      <td>-0.893147</td>\n",
              "      <td>0.982163</td>\n",
              "      <td>-0.999566</td>\n",
              "      <td>0.996798</td>\n",
              "      <td>0.977021</td>\n",
              "      <td>0.915342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>101004113630</td>\n",
              "      <td>0.175361</td>\n",
              "      <td>0.668518</td>\n",
              "      <td>-3.556810</td>\n",
              "      <td>-1.355421</td>\n",
              "      <td>1.444425</td>\n",
              "      <td>0.603148</td>\n",
              "      <td>-1.361185</td>\n",
              "      <td>1.510217</td>\n",
              "      <td>-0.073560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.120324</td>\n",
              "      <td>-0.999417</td>\n",
              "      <td>0.059866</td>\n",
              "      <td>-0.999934</td>\n",
              "      <td>-0.765923</td>\n",
              "      <td>0.960436</td>\n",
              "      <td>-0.998330</td>\n",
              "      <td>0.985888</td>\n",
              "      <td>0.973252</td>\n",
              "      <td>0.943457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>101004113228</td>\n",
              "      <td>-0.649289</td>\n",
              "      <td>0.929046</td>\n",
              "      <td>-2.988562</td>\n",
              "      <td>-1.767200</td>\n",
              "      <td>-0.438132</td>\n",
              "      <td>-0.361119</td>\n",
              "      <td>0.010478</td>\n",
              "      <td>1.114518</td>\n",
              "      <td>-0.448996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183652</td>\n",
              "      <td>-0.997964</td>\n",
              "      <td>0.057766</td>\n",
              "      <td>-0.999915</td>\n",
              "      <td>-0.724165</td>\n",
              "      <td>0.798789</td>\n",
              "      <td>-0.999548</td>\n",
              "      <td>0.995305</td>\n",
              "      <td>0.995679</td>\n",
              "      <td>0.948751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>101004113229</td>\n",
              "      <td>-0.649289</td>\n",
              "      <td>0.929046</td>\n",
              "      <td>-2.988562</td>\n",
              "      <td>-1.767200</td>\n",
              "      <td>-0.438132</td>\n",
              "      <td>-0.361119</td>\n",
              "      <td>0.010478</td>\n",
              "      <td>1.114518</td>\n",
              "      <td>-0.448996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183652</td>\n",
              "      <td>-0.997964</td>\n",
              "      <td>0.057766</td>\n",
              "      <td>-0.999915</td>\n",
              "      <td>-0.724165</td>\n",
              "      <td>0.798789</td>\n",
              "      <td>-0.999548</td>\n",
              "      <td>0.995305</td>\n",
              "      <td>0.995679</td>\n",
              "      <td>0.948751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   case_enquiry_id  desc_cls_0  desc_cls_1  desc_cls_2  desc_cls_3  \\\n",
              "0     101004113559   -1.100319    0.180848   -3.067521   -2.449431   \n",
              "1     101004113295   -0.136960    0.691521   -3.540846   -1.352687   \n",
              "2     101004113630    0.175361    0.668518   -3.556810   -1.355421   \n",
              "3     101004113228   -0.649289    0.929046   -2.988562   -1.767200   \n",
              "4     101004113229   -0.649289    0.929046   -2.988562   -1.767200   \n",
              "\n",
              "   desc_cls_4  desc_cls_5  desc_cls_6  desc_cls_7  desc_cls_8  ...  \\\n",
              "0    0.048062    0.750774   -1.156688    1.701071   -1.121157  ...   \n",
              "1    1.299200   -0.141181    0.158119    2.410162    0.071449  ...   \n",
              "2    1.444425    0.603148   -1.361185    1.510217   -0.073560  ...   \n",
              "3   -0.438132   -0.361119    0.010478    1.114518   -0.448996  ...   \n",
              "4   -0.438132   -0.361119    0.010478    1.114518   -0.448996  ...   \n",
              "\n",
              "   code_pooled_118  code_pooled_119  code_pooled_120  code_pooled_121  \\\n",
              "0         0.121683        -0.998929         0.036228        -0.999963   \n",
              "1         0.086003        -0.998614         0.014076        -0.999995   \n",
              "2         0.120324        -0.999417         0.059866        -0.999934   \n",
              "3         0.183652        -0.997964         0.057766        -0.999915   \n",
              "4         0.183652        -0.997964         0.057766        -0.999915   \n",
              "\n",
              "   code_pooled_122  code_pooled_123  code_pooled_124  code_pooled_125  \\\n",
              "0        -0.742293         0.969737        -0.998525         0.986848   \n",
              "1        -0.893147         0.982163        -0.999566         0.996798   \n",
              "2        -0.765923         0.960436        -0.998330         0.985888   \n",
              "3        -0.724165         0.798789        -0.999548         0.995305   \n",
              "4        -0.724165         0.798789        -0.999548         0.995305   \n",
              "\n",
              "   code_pooled_126  code_pooled_127  \n",
              "0         0.982758         0.931549  \n",
              "1         0.977021         0.915342  \n",
              "2         0.973252         0.943457  \n",
              "3         0.995679         0.948751  \n",
              "4         0.995679         0.948751  \n",
              "\n",
              "[5 rows x 769 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2267839, 269)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydata = mydata.drop_duplicates(subset=['case_enquiry_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2267839, 269)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#join them so we are left only with records that have mydata in both files\n",
        "new_mydata = mydata.merge(df, on='case_enquiry_id', how='inner')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(201522, 1037)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_mydata = new_mydata.sort_values(by='case_enquiry_id')\n",
        "new_mydata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_bin_edges = [0, 12, 24, 72, 168, 336, 672, 1344, 2688, 9999999]\n",
        "old_bin_labels = [\n",
        "                \"0-12 hours\",      # Less than half a day\n",
        "                \"12-24 hours\",     # Half to one day\n",
        "                \"1-3 days\",        # One to three days\n",
        "                \"4-7 days\",        # Four to seven days\n",
        "                \"1-2 weeks\",       # One to two weeks\n",
        "                \"2-4 weeks\",       # Two to four weeks\n",
        "                \"1-2 months\",      # One to two months\n",
        "                \"2-4 months\",      # Two to four months\n",
        "                \"4+ months\"        # More than four months\n",
        "            ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_time_bins(hour_interval, max_days, overflow_label=None):\n",
        "    # Calculate the number of hours for max_days\n",
        "    max_hours = max_days * 24\n",
        "    \n",
        "    # Generate bin edges\n",
        "    bin_edges = [i for i in range(0, max_hours + 1, hour_interval)]\n",
        "    bin_edges.append(1000000)  # for the overflow category\n",
        "    \n",
        "    # Generate bin labels\n",
        "    bin_labels = []\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        start_day = bin_edges[i] // 24\n",
        "        end_day = (bin_edges[i + 1] // 24) - 1  # -1 because it's inclusive\n",
        "        if end_day > start_day:\n",
        "            bin_labels.append(f\"{start_day}-{end_day} days\")\n",
        "        else:\n",
        "            bin_labels.append(f\"{start_day} days\")\n",
        "    \n",
        "    if overflow_label is not None:\n",
        "        bin_labels[-1] = overflow_label  # update the last label to the overflow label if specified\n",
        "\n",
        "    return bin_edges, bin_labels\n",
        "\n",
        "# Example usage\n",
        "hour_interval = 72\n",
        "max_days = 180\n",
        "bin_edges, bin_labels = generate_time_bins(hour_interval, max_days, \"180+ days\")\n",
        "bin_number = len(bin_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$prediction_timespans = [\n",
            "    \"0-2 days\" => [0, 72],\n",
            "    \"3-5 days\" => [72, 144],\n",
            "    \"6-8 days\" => [144, 216],\n",
            "    \"9-11 days\" => [216, 288],\n",
            "    \"12-14 days\" => [288, 360],\n",
            "    \"15-17 days\" => [360, 432],\n",
            "    \"18-20 days\" => [432, 504],\n",
            "    \"21-23 days\" => [504, 576],\n",
            "    \"24-26 days\" => [576, 648],\n",
            "    \"27-29 days\" => [648, 720],\n",
            "    \"30-32 days\" => [720, 792],\n",
            "    \"33-35 days\" => [792, 864],\n",
            "    \"36-38 days\" => [864, 936],\n",
            "    \"39-41 days\" => [936, 1008],\n",
            "    \"42-44 days\" => [1008, 1080],\n",
            "    \"45-47 days\" => [1080, 1152],\n",
            "    \"48-50 days\" => [1152, 1224],\n",
            "    \"51-53 days\" => [1224, 1296],\n",
            "    \"54-56 days\" => [1296, 1368],\n",
            "    \"57-59 days\" => [1368, 1440],\n",
            "    \"60-62 days\" => [1440, 1512],\n",
            "    \"63-65 days\" => [1512, 1584],\n",
            "    \"66-68 days\" => [1584, 1656],\n",
            "    \"69-71 days\" => [1656, 1728],\n",
            "    \"72-74 days\" => [1728, 1800],\n",
            "    \"75-77 days\" => [1800, 1872],\n",
            "    \"78-80 days\" => [1872, 1944],\n",
            "    \"81-83 days\" => [1944, 2016],\n",
            "    \"84-86 days\" => [2016, 2088],\n",
            "    \"87-89 days\" => [2088, 2160],\n",
            "    \"90-92 days\" => [2160, 2232],\n",
            "    \"93-95 days\" => [2232, 2304],\n",
            "    \"96-98 days\" => [2304, 2376],\n",
            "    \"99-101 days\" => [2376, 2448],\n",
            "    \"102-104 days\" => [2448, 2520],\n",
            "    \"105-107 days\" => [2520, 2592],\n",
            "    \"108-110 days\" => [2592, 2664],\n",
            "    \"111-113 days\" => [2664, 2736],\n",
            "    \"114-116 days\" => [2736, 2808],\n",
            "    \"117-119 days\" => [2808, 2880],\n",
            "    \"120-122 days\" => [2880, 2952],\n",
            "    \"123-125 days\" => [2952, 3024],\n",
            "    \"126-128 days\" => [3024, 3096],\n",
            "    \"129-131 days\" => [3096, 3168],\n",
            "    \"132-134 days\" => [3168, 3240],\n",
            "    \"135-137 days\" => [3240, 3312],\n",
            "    \"138-140 days\" => [3312, 3384],\n",
            "    \"141-143 days\" => [3384, 3456],\n",
            "    \"144-146 days\" => [3456, 3528],\n",
            "    \"147-149 days\" => [3528, 3600],\n",
            "    \"150-152 days\" => [3600, 3672],\n",
            "    \"153-155 days\" => [3672, 3744],\n",
            "    \"156-158 days\" => [3744, 3816],\n",
            "    \"159-161 days\" => [3816, 3888],\n",
            "    \"162-164 days\" => [3888, 3960],\n",
            "    \"165-167 days\" => [3960, 4032],\n",
            "    \"168-170 days\" => [4032, 4104],\n",
            "    \"171-173 days\" => [4104, 4176],\n",
            "    \"174-176 days\" => [4176, 4248],\n",
            "    \"177-179 days\" => [4248, 4320],\n",
            "    \"180+ days\" => [4320, 1000000],\n",
            "];\n"
          ]
        }
      ],
      "source": [
        "php_array = \"$prediction_timespans = [\\n\"\n",
        "for i, label in enumerate(bin_labels):\n",
        "    try:\n",
        "        new_line = f'    \"{label}\" => [{bin_edges[i]}, {bin_edges[i+1]}],\\n'\n",
        "        php_array += new_line\n",
        "    except IndexError:\n",
        "        continue\n",
        "php_array += \"];\"\n",
        "\n",
        "print(php_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df, y = kerasNLP_model.split_data(new_mydata, bin_edges=bin_edges, bin_labels=bin_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cast all columns that are type bool to float\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'bool':\n",
        "        df[col] = df[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(201522, 1034)\n",
            "(201522,)\n"
          ]
        }
      ],
      "source": [
        "#list the number of rows in X and y\n",
        "print(df.shape)\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#best_model, best_hyperparameters = kerasNLP_model.tune_model(df, y, '/home/briarmoss/Documents/Boston_311/models/tuning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define hyperparameters\n",
        "from kerastuner import HyperParameters\n",
        "\n",
        "#set constants\n",
        "start_nodes = 1024  \n",
        "end_nodes = 256\n",
        "#l2_0 = 0.00001\n",
        "#learning_rate = 7.5842e-05\n",
        "l2_0 = 0.001\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "hp = HyperParameters()\n",
        "hp.Fixed('start_nodes', start_nodes)\n",
        "hp.Fixed('end_nodes', end_nodes)\n",
        "hp.Fixed('l2_0', l2_0)\n",
        "hp.Fixed('learning_rate', learning_rate)\n",
        "hp.Fixed('final_layer', bin_number)\n",
        "hp.Fixed('final_activation', 'softmax')\n",
        "kerasNLP_model.best_hyperparameters = hp\n",
        "\n",
        "\n",
        "#parameters for linear regression\n",
        "linear='''\n",
        "hp = HyperParameters()\n",
        "hp.Fixed('start_nodes', start_nodes)\n",
        "hp.Fixed('end_nodes', end_nodes)\n",
        "hp.Fixed('l2_0', l2_0)\n",
        "hp.Fixed('learning_rate', learning_rate)\n",
        "hp.Fixed('final_layer', 1)\n",
        "hp.Fixed('final_activation', 'linear')\n",
        "kerasNLP_model.best_hyperparameters = hp\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "#free all unused dataframes\n",
        "try :\n",
        "    df_to_delete = [X, new_mydata, is_numeric, mydata, merged_data]\n",
        "    df_to_delete.extend([df_desc_cls, df_desc_pooled, df_name_cls, df_name_pooled, df_code_cls, df_code_pooled])\n",
        "except NameError:\n",
        "    pass\n",
        "try :\n",
        "    for data_frame in df_to_delete:\n",
        "        try:\n",
        "            del data_frame\n",
        "        #if the dataframe doesn't exist, pass\n",
        "        except NameError:\n",
        "            pass\n",
        "except NameError:\n",
        "    pass\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Suppress specific TensorFlow log messages\n",
        "logging.getLogger('tensorflow').addFilter(\n",
        "    lambda record: \"ROCm Fusion is enabled\" not in record.msg\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training at 2023-10-21 01:07:15.092239\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1024)              1059840   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 61)                15677     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1731645 (6.61 MB)\n",
            "Trainable params: 1731645 (6.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'> (161217, 61)\n",
            "<class 'pandas.core.frame.DataFrame'> (40305, 61)\n",
            "run fit\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 01:07:16.637040: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1333587024 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5037/5039 [============================>.] - ETA: 0s - loss: 1.6206 - accuracy: 0.7822 - top_k_categorical_accuracy: 0.8520"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 01:08:10.198495: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 333402960 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5039/5039 [==============================] - 54s 11ms/step - loss: 1.6205 - accuracy: 0.7822 - top_k_categorical_accuracy: 0.8520 - val_loss: 1.4757 - val_accuracy: 0.7256 - val_top_k_categorical_accuracy: 0.8169\n",
            "Epoch 2/10\n",
            "5039/5039 [==============================] - 53s 10ms/step - loss: 1.1146 - accuracy: 0.7946 - top_k_categorical_accuracy: 0.8585 - val_loss: 1.3290 - val_accuracy: 0.7410 - val_top_k_categorical_accuracy: 0.8197\n",
            "Epoch 3/10\n",
            "5039/5039 [==============================] - 53s 10ms/step - loss: 1.0096 - accuracy: 0.7963 - top_k_categorical_accuracy: 0.8598 - val_loss: 1.3290 - val_accuracy: 0.7220 - val_top_k_categorical_accuracy: 0.8140\n",
            "Epoch 4/10\n",
            "5039/5039 [==============================] - 52s 10ms/step - loss: 0.9669 - accuracy: 0.7977 - top_k_categorical_accuracy: 0.8604 - val_loss: 1.2324 - val_accuracy: 0.7399 - val_top_k_categorical_accuracy: 0.8185\n",
            "Epoch 5/10\n",
            "5039/5039 [==============================] - 53s 10ms/step - loss: 0.9454 - accuracy: 0.7978 - top_k_categorical_accuracy: 0.8607 - val_loss: 1.2613 - val_accuracy: 0.7281 - val_top_k_categorical_accuracy: 0.8149\n",
            "Epoch 6/10\n",
            "5039/5039 [==============================] - 53s 10ms/step - loss: 0.9313 - accuracy: 0.7989 - top_k_categorical_accuracy: 0.8614 - val_loss: 1.2549 - val_accuracy: 0.7220 - val_top_k_categorical_accuracy: 0.8157\n",
            "Epoch 7/10\n",
            "5039/5039 [==============================] - 53s 11ms/step - loss: 0.9240 - accuracy: 0.7987 - top_k_categorical_accuracy: 0.8613 - val_loss: 1.2330 - val_accuracy: 0.7242 - val_top_k_categorical_accuracy: 0.8154\n",
            "Epoch 8/10\n",
            "5039/5039 [==============================] - 53s 11ms/step - loss: 0.9167 - accuracy: 0.7998 - top_k_categorical_accuracy: 0.8619 - val_loss: 1.2336 - val_accuracy: 0.7202 - val_top_k_categorical_accuracy: 0.8135\n",
            "Epoch 9/10\n",
            "5039/5039 [==============================] - 54s 11ms/step - loss: 0.9120 - accuracy: 0.7995 - top_k_categorical_accuracy: 0.8616 - val_loss: 1.2470 - val_accuracy: 0.7189 - val_top_k_categorical_accuracy: 0.8138\n",
            "Epoch 10/10\n",
            "5039/5039 [==============================] - 54s 11ms/step - loss: 0.9075 - accuracy: 0.7999 - top_k_categorical_accuracy: 0.8619 - val_loss: 1.2141 - val_accuracy: 0.7300 - val_top_k_categorical_accuracy: 0.8175\n",
            " 123/1260 [=>............................] - ETA: 1s - loss: 1.2452 - accuracy: 0.7315 - top_k_categorical_accuracy: 0.8194"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-21 01:16:08.222713: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 333402960 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1260/1260 [==============================] - 2s 1ms/step - loss: 1.2141 - accuracy: 0.7300 - top_k_categorical_accuracy: 0.8175\n",
            "Testing accuracy: 0.7300086617469788 \n",
            "Top-2 accuracy: 0.8174667954444885 \n",
            "Test loss: 1.2141273021697998\n",
            "Ending Training at 2023-10-21 01:16:10.078329\n",
            "Training took 0:08:54.986090\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#parse CLS embedding column as array\n",
        "test_acc = kerasNLP_model.train_model( df, y )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSSkyth-2K3A"
      },
      "source": [
        "## Train several models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning is fun!\n"
          ]
        }
      ],
      "source": [
        "print(\"learning is fun!\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv4ivBuvinss",
        "outputId": "814ef4fb-e100-4b22-ccbc-6b9d91e5a341"
      },
      "outputs": [],
      "source": [
        "#logistic_tree_model.run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UIaks6jjPkU",
        "outputId": "0e365ed1-472e-4cb1-cdde-ae0a2d32722e"
      },
      "outputs": [],
      "source": [
        "#logistic_model.run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1368"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "kerasNLP_model.best_hyperparameters = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/briarmoss/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "def save_model_to_dir(model, folder_name):\n",
        "    dir_path = os.path.join(MODEL_FOLDER, folder_name)\n",
        "    \n",
        "    if not os.path.exists(dir_path):\n",
        "        os.mkdir(dir_path)\n",
        "    \n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    model_name = timestamp + \"_\" + model.model_type\n",
        "    properties_name = model_name\n",
        "    \n",
        "    model.save(dir_path, model_name, properties_name)\n",
        "\n",
        "# List of models\n",
        "models = [kerasNLP_model]\n",
        "\n",
        "\n",
        "# Iterate over models and save\n",
        "for model in models:\n",
        "    save_model_to_dir(model, model.model_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndata = kerasNLP_model.load_data( 'predict' )\\ndata = kerasNLP_model.enhance_data( data, 'predict')\\nclean_data = kerasNLP_model.clean_data_for_prediction( data )\\n\\nX_predict, y_predict = kerasNLP_model.split_data( clean_data )\\ny_predict = kerasNLP_model.model.predict(X_predict)\\ndata['survival_prediction'] = y_predict\\nreturn data\\n\""
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "data = kerasNLP_model.load_data( 'predict' )\n",
        "data = kerasNLP_model.enhance_data( data, 'predict')\n",
        "clean_data = kerasNLP_model.clean_data_for_prediction( data )\n",
        "\n",
        "X_predict, y_predict = kerasNLP_model.split_data( clean_data )\n",
        "y_predict = kerasNLP_model.model.predict(X_predict)\n",
        "data['survival_prediction'] = y_predict\n",
        "return data\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
