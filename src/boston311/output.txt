./Boston311EventDecTree.py
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from datetime import datetime
import pickle 
from .Boston311Model import Boston311Model

class Boston311EventDecTree(Boston311Model):


    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def save(self, filepath, model_file, properties_file):
                
        with open(filepath + '/' + model_file + '.pkl', 'wb') as f:
            pickle.dump(self.model, f)
       
        # Save other properties
        super().save_properties(filepath, properties_file)

    def load(self, json_file, model_file):

        # Load other properties
        super().load_properties(json_file)
        
        with open(model_file, 'rb') as f:
            self.model = pickle.load(f)
    
    def predict( self, data=None ) :
        if data is None :
            data = self.load_data( train_or_predict='predict' )
        else :
            data = self.load_data( data=data, train_or_predict='predict' )
        data = self.enhance_data( data, 'predict')
        clean_data = self.clean_data_for_prediction( data )

        X_predict, y_predict = self.split_data( clean_data )
        y_predict = self.model.predict(X_predict)
        data.insert(0, 'event_prediction', y_predict)
        return data
    
    def split_data(self, data) :
        X = data.drop(['survival_time_hours', 'event'], axis=1) 
        y = data['event']

        return X, y 
        
    def train_model( self, X, y=[] ) :
        test_accuracy = 0
        self.model, test_accuracy = self.train_tree_model( X, y )
        return test_accuracy

    def train_tree_model ( self, tree_X, tree_y ) :
        start_time = datetime.now()
        print("Starting Training at {}".format(start_time))

        # Split into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(tree_X, tree_y, test_size=0.2, random_state=42)

        # Initialize the model
        model = DecisionTreeClassifier(random_state=42)

        # Fit the model
        model.fit(X_train, y_train)

        y_test_pred = model.predict(X_test)

        # Calculate the accuracy on the testing set
        test_accuracy = accuracy_score(y_test, y_test_pred)
        print('Testing accuracy:', test_accuracy)

        end_time = datetime.now()
        total_time = (end_time - start_time)
        print("Ending Training at {}".format(end_time))
        print("Training took {}".format(total_time))

        return model, test_accuracy
    
    def run_pipeline( self, data=None) :
        if data is None :
            data = self.load_data()
        else :
            data = self.load_data( data=data )
        data = self.enhance_data(data)
        data = self.apply_scenario(data)
        data = self.clean_data(data)
        X, y = self.split_data(data)
        test_accuracy = self.train_model( X, y )
        return test_accuracy./Boston311KerasNN.py
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from datetime import datetime
import pandas as pd
import numpy as np
import pickle 
import json
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.metrics import TopKCategoricalAccuracy
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow import keras
from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization
from kerastuner import HyperParameters
from .Boston311Model import Boston311Model

class Boston311KerasNN(Boston311Model):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.best_hyperparameters = None
        self.input_dim = None
        self.batch_size = kwargs.get('batch_size', 32)
        self.patience = kwargs.get('patience', 5)
        self.api_data = kwargs.get('api_data', None)
        self.bin_edges = kwargs.get('bin_edges', None)
        self.bin_labels = kwargs.get('bin_labels', None)
        self.epochs = kwargs.get('epochs', None)


    def save(self, filepath, model_file, properties_file):
        # Save keras model
        self.model.save(filepath + '/' + model_file + '.keras')

        with open(filepath + '/' + properties_file + '.json', 'w') as f:
            save_dict = {
                'feature_columns': self.feature_columns,
                'feature_dict': self.feature_dict,
                'train_date_range': self.train_date_range,
                'predict_date_range': self.predict_date_range,
                'scenario': self.scenario,
                'bin_edges': self.bin_edges,
                'bin_labels': self.bin_labels,
                'epochs': self.epochs,
                'patience': self.patience,
                'batch_size': self.batch_size,
                'input_dim': self.input_dim
            }
            if self.best_hyperparameters is not None :
                save_dict['best_hyperparameters'] = self.best_hyperparameters.get_config()

            json.dump(save_dict, f)


    def load(self, json_file, model_file):
        # Load other properties
        with open(json_file, 'r') as f:
            properties = json.load(f)
            self.feature_columns = properties['feature_columns']
            self.feature_dict = properties['feature_dict']
            self.train_date_range = properties['train_date_range']
            self.predict_date_range = properties['predict_date_range']
            self.scenario = properties['scenario']
            #check if properties has a best_hyperparameters attribute, and if so, load it
            if 'best_hyperparameters' in properties and properties['best_hyperparameters'] is not None:
                self.best_hyperparameters = HyperParameters.from_config(properties['best_hyperparameters'])
    
        self.model = keras.models.load_model(model_file)

    def load_data(self, data=None, train_or_predict='train') :
        return super().load_data(data, train_or_predict)
    
    def enhance_data(self, data, train_or_predict='train'):
        return super().enhance_data(data, train_or_predict)
    
    def apply_scenario(self, data):
        return super().apply_scenario(data)
    
    def clean_data(self, data):
        data = super().clean_data(data)
        for col in data.columns:
            if data[col].dtype == 'bool':
                data[col] = data[col].astype('float64')
        return data
    
    def add_api_data(self, data, api_data):
        data = data.drop_duplicates(subset=['case_enquiry_id'])
        api_data = api_data.drop_duplicates(subset=['case_enquiry_id'])
        data = data.merge(api_data, on='case_enquiry_id', how='inner')
        return data
    
    def clean_data_for_prediction(self, data):
        data = super().clean_data_for_prediction(data)
        for col in data.columns:
            if data[col].dtype == 'bool':
                data[col] = data[col].astype('float64')
        return data
    
    def one_hot_encode_with_feature_dict(self, data):
        return super().one_hot_encode_with_feature_dict(data)
    
    def predict( self, api_data=None, data=None ) :
        if data is None :
            data = self.load_data( train_or_predict='predict' )
        else :
            data = self.load_data( data, train_or_predict='predict' )
        data = self.enhance_data( data, 'predict')
        clean_data = self.clean_data_for_prediction( data )
        if api_data is not None :
            clean_data = self.add_api_data(clean_data, api_data)
            data_limited = data[data['case_enquiry_id'].isin(api_data['case_enquiry_id'])]
        else :
            data_limited = data
        
        X_predict, y_predict = self.split_data( clean_data, self.bin_labels, self.bin_edges )
        y_predict = self.model.predict(X_predict)
        
        return y_predict, data_limited 
    
    
    def flatten_and_replace_columns(self, df, column_names):
        new_dfs = []
        
        # Flatten and remove original columns
        for col_name in column_names:
            flattened = np.stack(df[col_name].to_numpy())
            new_df = pd.DataFrame(flattened, columns=[f'{col_name}_{i}' for i in range(flattened.shape[1])])
            new_dfs.append(new_df)
            df.drop([col_name], axis=1, inplace=True)
            
        # Concatenate new columns to the original DataFrame
        df = pd.concat([df] + new_dfs, axis=1)
        return df
    
    def split_data(self, data, bin_labels=None, bin_edges=None) :

        X = data.drop(['survival_time_hours', 'event'], axis=1)
        #if X has a 'case_enquiry_id' column, drop it
        if 'case_enquiry_id' in X.columns :
            X = X.drop(['case_enquiry_id'], axis=1)
        
        if bin_edges is None :
            #keep y as survival_time_hours for regression
            y = data['survival_time_hours']
        else :
            y = pd.cut(data['survival_time_hours'], bins=bin_edges, labels=bin_labels)
        
        return X, y 
        
    def train_model( self, X, y=[], start_nodes=128, end_nodes=64, final_layer_choice=9, final_activation_choice='softmax', epochs=10 ) :
        test_accuracy = 0
        self.model, test_accuracy = self.train_keras_model( X, y, start_nodes, end_nodes, final_layer_choice, final_activation_choice, my_epochs=epochs )
        return test_accuracy

    def train_keras_model ( self, tree_X, tree_y, start_nodes=128, end_nodes=64, final_layer_choice=9, final_activation_choice='softmax', my_epochs=10 ) :
        start_time = datetime.now()
        print("Starting Training at {}".format(start_time))

        self.input_dim = tree_X.shape[1]
        print("input_dim: {}".format(self.input_dim))
        

        # Split into training and testing sets
        #X_train, X_test, y_train, y_test = train_test_split(tree_X, tree_y, test_size=0.2, random_state=42)

        # Calculate the index for the split
        split_index = int(0.6 * len(tree_X))
        ###
        # Create training and testing sets
        #X_train = tree_X.iloc[:split_index]
        #y_train = tree_y.iloc[:split_index]

        # Make them leaky instead
        #X_train = tree_X
        #y_train = tree_y

        #X_test = tree_X.iloc[split_index:]
        #y_test = tree_y.iloc[split_index:]
        ###

        # Define indices
        indices = np.arange(len(tree_X))

        # For training, take all but every 5th and 4th case
        train_idx = indices[(indices % 5 != 0)]
        X_train = tree_X.iloc[train_idx]
        y_train = tree_y.iloc[train_idx]

        # For validation, take every 4th case
        #val_idx = indices[indices % 5 == 1]
        #X_val = tree_X.iloc[val_idx]
        #y_val = tree_y.iloc[val_idx]

        X_val = tree_X.iloc[split_index:]
        y_val = tree_y.iloc[split_index:]

        # For testing, take every 5th case
        test_idx = indices[indices % 5 == 0]
        X_test = tree_X.iloc[test_idx]
        y_test = tree_y.iloc[test_idx]


        if self.best_hyperparameters is not None:
            model = self.build_model(self.best_hyperparameters)
        else:
            hp = HyperParameters()
            hp.Fixed('start_nodes', value=start_nodes)
            hp.Fixed('end_nodes', value=end_nodes)
            hp.Fixed('final_layer', value=final_layer_choice)
            hp.Fixed('l2_0', value=0.00001)
            hp.Fixed('learning_rate', value=7.5842e-05)
            hp.Fixed('final_activation', value=final_activation_choice)

            # Build the model with the specific hyperparameters
            model = self.build_model(hp)

        print(model.summary())

        #Add Early Stopping
        early_stopping = EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)

        # Fit the model
        y_train = pd.get_dummies(y_train)
        y_val = pd.get_dummies(y_val)

        #print debug
        print(type(y_train), y_train.shape)

        y_test = pd.get_dummies(y_test)

        #print debug
        print(type(y_test), y_test.shape)

        print("run fit\n")

        model.fit(X_train, y_train, epochs=my_epochs, batch_size=self.batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping])

        # Evaluate the model
        test_loss, test_accuracy, top2_accuracy = model.evaluate(X_test, y_test)
        print('Testing accuracy:', test_accuracy, '\nTop-2 accuracy:', top2_accuracy, '\nTest loss:', test_loss)

        end_time = datetime.now()
        total_time = (end_time - start_time)
        print("Ending Training at {}".format(end_time))
        print("Training took {}".format(total_time))

        return model, test_accuracy
    
    def run_pipeline( self, data=None, api_data=None) :
        if self.bin_edges is None :
            print("bin_edges is None")  
            self.bin_edges = self.generate_time_bins_fixed_interval(24, 180)
            self.bin_labels = self.generate_bin_labels(self.bin_edges, "over 6 months")
        if self.bin_labels is None :
            print("bin_labels is None")
            last_edge = self.bin_edges[-1]
            overflow_label = self.time_format(last_edge)
            self.bin_labels = self.generate_bin_labels(self.bin_edges, "over " + overflow_label)
        if data is None :
            data = self.load_data()
        else :
            data = self.load_data(data=data)
        data = self.enhance_data(data)
        data = self.apply_scenario(data)
        data = self.clean_data(data)
        if api_data is not None :
            data = self.add_api_data(data, api_data)
        #sort before split_data so train_model can take every 5th case for testing
        data = data.sort_values(by='case_enquiry_id')
        X, y = self.split_data(data, self.bin_labels, self.bin_edges)
        if self.epochs is None :
            self.epochs = 2
        test_accuracy = self.train_model( X, y, epochs=self.epochs )
        return test_accuracy
    
    def tune_model( self, X_train, y_train, model_dir, verboseLevel=1):
        print(type(X_train), X_train.shape)
        print(type(y_train), y_train.shape)
        y_train = pd.get_dummies(y_train)
        print(type(y_train), y_train.shape)

        self.input_dim = X_train.shape[1]
        tuner = BayesianOptimization(
            hypermodel=self.build_model,
            objective='val_accuracy',
            max_trials=300,
            num_initial_points=10,
            directory=model_dir,
            project_name='keras_tuning',
            overwrite=True
        )

        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

        tuner.search(X_train_split, y_train_split,
                    epochs=10,
                    validation_data=(X_val_split, y_val_split),
                    verbose=verboseLevel)
        
        best_model = tuner.get_best_models(num_models=1)[0]
        best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
        
        return best_model, best_hyperparameters

    def build_model( self, hp):
        model = Sequential()

        # Assuming input shape is (steps, input_dim)
        #model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=self.input_dim))
        #model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
        #model.add(Flatten())  # Flatten the sequence to 1D array for the Dense layer

        start_nodes_choice = hp.Choice(f'start_nodes', [128, 256, 512, 1024])
        end_nodes_choice = hp.Choice(f'end_nodes', [16, 32, 64])
        final_layer_choice = hp.Choice(f'final_layer', [16, 32, 64])
        final_activation_choice = hp.Choice(f'final_activation', ['softmax', 'linear'])
        model.add(Dense(start_nodes_choice, input_dim=self.input_dim, activation='relu', kernel_regularizer=l2(hp.Float('l2_0', min_value=1e-5, max_value=1e-1, sampling='LOG'))))
        
        #if hp.Choice(f'batch_normalization', [True, False]):
        #    model.add(BatchNormalization())

        #create a loop to add layers of half the size of the previous layer until the layer size is end_nodes_choice:
        while start_nodes_choice > end_nodes_choice: 
            start_nodes_choice = start_nodes_choice // 2 
            model.add(Dense(start_nodes_choice, activation='relu', kernel_regularizer=l2(hp.Float('l2_0', min_value=1e-5, max_value=1e-1, sampling='LOG'))))
            #add a dropout of .2
            #model.add(Dropout(0.2))
                
        model.add(Dense(final_layer_choice, activation=final_activation_choice))
        
        top2_acc = TopKCategoricalAccuracy(k=2)
        optimizer = Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-1, sampling='LOG'))
        
        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', top2_acc])
        
        return model
./Boston311LinReg.py
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from datetime import datetime
import pandas as pd
from .Boston311Model import Boston311Model

class Boston311LinReg(Boston311Model):

    def save(self, filepath, model_file, properties_file):
        # Save keras model
        self.model.save(filepath + '/' + model_file + '.h5')
        
        # Save other properties
        super().save_properties(filepath, properties_file)

    def load(self, json_file, model_file):

        # Load other properties
        super().load_properties(json_file)
        self.model = keras.models.load_model(model_file)
    
    def predict( self, data=None ) :
        if data is None :
            data = self.load_data( train_or_predict='predict' )
        else :
            data = self.load_data( data=data, train_or_predict='predict' )
        data = self.enhance_data( data, 'predict')
        clean_data = self.clean_data_for_prediction( data )

        X_predict, y_predict = self.split_data( clean_data )
        y_predict = self.model.predict(X_predict)
        # Insert survival_timedelta at the leftmost side of the DataFrame
        data.insert(0, 'survival_timedelta', data['survival_prediction'].apply(lambda x: pd.Timedelta(seconds=(x*3600))))

        # Since survival_timedelta depends on survival_prediction, we insert survival_prediction next
        data.insert(0, 'survival_prediction', y_predict)

        # Insert closed_dt_prediction at the leftmost side of the DataFrame
        data.insert(0, 'closed_dt_prediction', data['open_dt'] + data['survival_timedelta'])

        return data
       
    def split_data(self, data) :
        X = data.drop(['survival_time_hours', 'event'], axis=1) 
        y = data['survival_time_hours']
        
        return X, y 
    
    def train_model( self, X, y=[] ) :
        self.model = self.train_linear_model( X, y )
        
    def train_linear_model( self, linear_X, linear_y ) :
        start_time = datetime.now()
        print("Starting Training at {}".format(start_time))

        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(linear_X) # scale the data
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, linear_y, test_size=0.2, random_state=42)

        # split the data again to create a validation set
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

        # define the model architecture
        model = keras.Sequential([
            keras.layers.Dense(units=1, input_dim=X_train.shape[1])
        ])

        # compile the model
        model.compile(optimizer='adam', loss='mean_squared_error')

        # train the model
        # we are adding early stopping based on the validation loss
        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='min')
        history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stop])

        end_time = datetime.now()
        total_time = (end_time - start_time)
        print("Ending Training at {}".format(end_time))
        print("Training took {}".format(total_time))

        return model
    
    def run_pipeline( self, data=None) :
        if data is None :
            data = self.load_data()
        else :
            data = self.load_data( data=data )
        data = self.enhance_data(data)
        data = self.apply_scenario(data)
        data = self.clean_data(data)
        X, y = self.split_data(data)
        self.train_model( X, y )
./Boston311LogReg.py
from tensorflow import keras
from sklearn.model_selection import train_test_split
from datetime import datetime
from .Boston311Model import Boston311Model

class Boston311LogReg(Boston311Model):

    def save(self, filepath, model_file, properties_file):
        # Save keras model
        self.model.save(filepath + '/' + model_file + '.keras')
        
        # Save other properties
        super().save_properties(filepath, properties_file)

    def load(self, json_file, model_file):

        # Load other properties
        super().load_properties(json_file)
    
        self.model = keras.models.load_model(model_file)
    
    def predict( self, data=None ) :
        if data is None :
            data = self.load_data( train_or_predict='predict' )
        else :
            data = self.load_data( data=data, train_or_predict='predict' )
        data = self.enhance_data( data, 'predict')
        clean_data = self.clean_data_for_prediction( data )

        X_predict, y_predict = self.split_data( clean_data )
        y_predict = self.model.predict(X_predict)
        data.insert(0, 'event_prediction', y_predict)
        return data
    
    def split_data(self, data) :

        X = data.drop(['survival_time_hours', 'event'], axis=1) 
        #if X has a 'case_enquiry_id' column, drop it
        if 'case_enquiry_id' in X.columns :
            X = X.drop(['case_enquiry_id'], axis=1)
        y = data['event']
        
        return X, y 
        
    def train_model( self, X, y=[] ) :
        test_acc = 0
        self.model, test_acc = self.train_logistic_model( X, y )
        return test_acc


    def train_logistic_model ( self, logistic_X, logistic_y ) :
        start_time = datetime.now()
        print("Starting Training at {}".format(start_time))

        # Split into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(logistic_X, logistic_y, test_size=0.2, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

        # Build model
        model = keras.Sequential([
            keras.layers.Dense(units=1, input_shape=(X_train.shape[1],), activation='sigmoid')
        ])

        # Compile model
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        # Define early stopping callback
        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)

        # Train model with early stopping
        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])

        # Evaluate model
        test_loss, test_acc = model.evaluate(X_test, y_test)

        print('Test accuracy:', test_acc)

        end_time = datetime.now()
        total_time = (end_time - start_time)
        print("Ending Training at {}".format(end_time))
        print("Training took {}".format(total_time))

        return model, test_acc
    
    def run_pipeline( self, data=None) :
        if data is None :
            data = self.load_data()
        else :
            data = self.load_data( data=data )
        data = self.enhance_data(data)
        data = self.apply_scenario(data)
        data = self.clean_data(data)
        X, y = self.split_data(data)
        test_acc = self.train_model( X, y )
        return test_acc./Boston311Model.py
import pandas as pd
import numpy as np
from math import pi
from math import cos
import json
import requests
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from pytz import timezone

#refactor code as separate classes for each model type  - linear, logistic, cox, decision tree

class Boston311Model: 
    
    '''
    model - our model once trained
    feature_columns - a list of our feature columns
    feature_dict - a dictionary with the keys being the names of our feature columns and the values being lists of all the possible values
    train_date_range - a dict with keys "start" and "end" and datetime values
    predict_date_range - a dict with keys "start" and "end" and datetime values
    scenario - our scenario data, maybe a list, maybe a dict, depending on how we recode our scenarios
    model_type - Our type of model, linear, logistic, etc
    '''
    def __init__(self, **kwargs) :
        self.model = kwargs.get('model', None) 
        self.feature_columns = kwargs.get('feature_columns', [])
        self.feature_dict = kwargs.get('feature_dict', {})
        self.train_date_range = kwargs.get('train_date_range', {'start':'2010-12-31', 'end':'2030-01-01'})
        self.predict_date_range = kwargs.get('predict_date_range', {'start':'', 'end':''})
        self.scenario = kwargs.get('scenario', {})
        self.files_dict = kwargs.get('files_dict', None)
        #add add_columns which will be a dataframe to add to the data
        self.add_columns = kwargs.get('add_columns', None)
        #self.model_type = kwargs.get('model_type', 'logistic')
        #define self.model_type as the class name of self 
        self.model_type = self.__class__.__name__
        self.bin_edges = kwargs.get('bin_edges', None)
        self.bin_labels = kwargs.get('bin_labels', None)



    def save_properties(self, filepath, properties_file):
        # Save other properties
        with open(filepath + '/' + properties_file + '.json', 'w') as f:
            json.dump({
                'feature_columns': self.feature_columns,
                'feature_dict': self.feature_dict,
                'train_date_range': self.train_date_range,
                'predict_date_range': self.predict_date_range,
                'scenario': self.scenario,
                'model_type': self.model_type
            }, f)


    def load_properties(self, json_file) :
        # Load other properties
        with open(json_file, 'r') as f:
            properties = json.load(f)
            self.feature_columns = properties['feature_columns']
            self.feature_dict = properties['feature_dict']
            self.train_date_range = properties['train_date_range']
            self.predict_date_range = properties['predict_date_range']
            self.scenario = properties['scenario']


    def get_datestrings(self, days=30) :
        now = datetime.now()
        days_timedelta = timedelta(days=days)
        X_days_ago = now - days_timedelta
        today_datestring = now.strftime("%Y-%m-%d")
        X_days_ago_datestring = X_days_ago.strftime("%Y-%m-%d")
        tomorrow_datestring = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')
        return today_datestring, tomorrow_datestring, X_days_ago_datestring
    
    #get current datetime in Boston timezone as string
    
    def get_current_datetime_str(self) :
        boston = timezone('US/Eastern')
        now = datetime.now(boston)
        today_datestring = now.strftime("%Y-%m-%d")
        #get time in Boston timezone as string for a filename
        now = datetime.now(boston)
        time_string = now.strftime("%H-%M-%S")
        #define datetime string
        my_datetime = today_datestring + '_' + time_string 
        return my_datetime
    
    #define a function that takes a path to a csv file and a pkl file and checks if the csv file is newer than the pkl file, and if so, loads the csv file into a dataframe and saves it as a pkl file, else loads the pkl file into a dataframe
    def pkl_load_data(self, csv_path, pkl_path):
        if os.path.exists(pkl_path):
            pkl_time = os.path.getmtime(pkl_path)
            csv_time = os.path.getmtime(csv_path)
            if csv_time > pkl_time:
                df = pd.read_csv(csv_path)
                df.to_pickle(pkl_path)
            else:
                df = pd.read_pickle(pkl_path)
        else:
            df = pd.read_csv(csv_path)
            df.to_pickle(pkl_path)
        return df



    #load_data() - this will use the start_date and end_date. It will return a dataframe
    def load_data(self, data=None, train_or_predict='train') :
        start_date, end_date = None, None
        if train_or_predict == 'train' :
            start_date = pd.to_datetime(self.train_date_range['start'])
            end_date = pd.to_datetime(self.train_date_range['end'])
        elif train_or_predict == 'predict' :
            start_date = pd.to_datetime(self.predict_date_range['start'])
            end_date = pd.to_datetime(self.predict_date_range['end'])
        if data is None :
            data = self.load_data_from_urls(range(start_date.year, end_date.year+1))
        else :
            data = data.copy()

        data['open_dt'] = pd.to_datetime(data['open_dt'])
        data = data[(data['open_dt'] >= start_date) & (data['open_dt'] <= end_date)]
            
        return data 

    
    #enhance_data( data ) - this will enhance the data according to our needs
    def enhance_data(self, data, train_or_predict='train') :
        
        data = data.copy()
        data['closed_dt'] = pd.to_datetime(data['closed_dt'])
        data['open_dt'] = pd.to_datetime(data['open_dt'])
        data['survival_time'] = data['closed_dt'] - data['open_dt']
        data['event'] = data['closed_dt'].notnull().astype(int)
        data['ward_number'] = data['ward'].str.extract(r'0*(\d+)')



        #add seasonality value
        #day_of_year = data['open_dt'].dt.dayofyear
        #data['season_cos'] = day_of_year.apply(lambda x: cos((x - 1) * (2. * pi / 365.25)))

        #add day of week
        #weekday = data['open_dt'].dt.weekday
        #data['weekday_cos'] = weekday.apply(lambda x: cos(x * (2. * pi / 7)))

        # initialize a new column with NaN values
        data['survival_time_hours'] = np.nan  

        # create a boolean mask for non-NaN values
        mask = data['survival_time'].notna()  
        data.loc[mask, 'survival_time_hours'] = data.loc[mask, 'survival_time'].apply(lambda x: x.total_seconds() / 3600)

        if train_or_predict == 'predict' :
            #drop closed cases - edit: Actually it would be better to keep all to see if we correctly identify the ones that are closed
            #data = data[(data['event'] == 0)]

            for key, value in self.scenario.items() :
              if key == 'dropColumnValues' :
                  for column, column_values in value.items() :
                      data = data[~data[column].isin(column_values)]
              if key == 'keepColumnValues' :
                  for column, column_values in value.items() :
                      data = data[data[column].isin(column_values)]

        return data
    
    def apply_scenario(self, data) :

        for key, value in self.scenario.items() :
            if key == 'dropColumnValues' :
                for column, column_values in value.items() :
                    data = data[~data[column].isin(column_values)]
            if key == 'keepColumnValues' :
                for column, column_values in value.items() :
                    data = data[data[column].isin(column_values)]
            if key == 'dropOpen' :
                data = data[(data['event'] == 1) | (data['open_dt'] < pd.to_datetime(value))]
            if key == 'survivalTimeMin' :
                delta = pd.Timedelta(seconds=value)
                data = data[(data['event'] == 0) | (data['survival_time'] >= delta)]
            if key == 'survivalTimeMax' :
                delta = pd.Timedelta(seconds=value)
                data = data[(data['event'] == 0) | (data['survival_time'] <= delta)]
            if key == 'eventToZeroforSurvivalTimeGreaterThan' :
                delta = pd.Timedelta(seconds=value)
                data.loc[(data['event'] == 1) & (data['survival_time'] > delta), 'event'] = 0
            if key == 'survivalTimeFill' :
                date = pd.to_datetime(value)
                # create a boolean mask for non-NaN values
                mask = data['survival_time'].isna() 
                data.loc[mask, 'survival_time'] = date - data.loc[mask, 'open_dt']
                data.loc[mask, 'survival_time_hours'] = data.loc[mask, 'survival_time'].apply(lambda x: x.total_seconds() / 3600)

        return data

    def clean_data(self, data) :

        #get a list of all columns not in feature_columns or our two labels
        cols_to_drop = data.columns.difference(self.feature_columns + ['event', 'survival_time_hours','case_enquiry_id'])


        data = data.drop(columns=cols_to_drop, axis=1)

        self.feature_dict = {}
        for column in self.feature_columns :
            self.feature_dict[column] = data[column].unique().tolist()
        
        data = pd.get_dummies(data, columns=self.feature_columns)


        return data



    
    #clean_data_for_prediction( data ) - this will drop any columns not in feature_columns, and one hot encode the training data for prediction with the model by using the feature_columns and feature_dict to ensure the cleaned data is in the correct format for prediction with this model.
    

    def clean_data_for_prediction( self, data ) :
        print("columns in data before drop:", data.columns)
        
        cols_to_drop = data.columns.difference(self.feature_columns + ['case_enquiry_id', 'event', 'survival_time_hours'])

        print("columns to drop:", cols_to_drop)

        data = data.drop(columns=cols_to_drop, axis=1)

        print("columns in data before ohewfd:", data.columns)

        data = self.one_hot_encode_with_feature_dict( data )

        return data

    def one_hot_encode_with_feature_dict( self, data ) :
        
        # Loop through each column in the DataFrame
        for column in data.columns:
            # Check if the column is case_enquiry_id and skip it
            if column in ['case_enquiry_id', 'event', 'survival_time_hours']:
                continue
            # Get the list of allowed values for this column
            allowed = self.feature_dict.get(column, [])
            
            # Loop through each value in the column
            for i, value in data[column].items():
                # Check if the value is in the list of allowed values
                if value not in allowed:
                    # Replace the value with a null value
                    data.at[i, column] = None
        
        fake_records = []
        for col, vals in self.feature_dict.items():
            missing_vals = set(vals) - set(data[col])
            for val in missing_vals:
                # create a dictionary with null values for all columns in your DataFrame
                fake_record = {col: None for col in data.columns}

                # update the dictionary with the non-null value for the current column
                fake_record[col] = val

                # append the fake record to the list of fake records
                fake_records.append(fake_record)

        fake_df = pd.DataFrame(fake_records)

        # Concatenate fake records with original data
        data = pd.concat([data, fake_df], ignore_index=True)

        # Get dummies and drop fake records
        dummies = pd.get_dummies(data, columns=self.feature_dict.keys())
        dummies = dummies.iloc[:-len(fake_df), :]

        return dummies
    
    def load_data_from_urls(self, *args) :
        url_2023 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/e6013a93-1321-4f2a-bf91-8d8a02f1e62f/download/tmpmbmp9j6w.csv"
        url_2022 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/81a7b022-f8fc-4da5-80e4-b160058ca207/download/tmph4izx_fb.csv"
        url_2021 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/f53ebccd-bc61-49f9-83db-625f209c95f5/download/tmppgq9965_.csv"
        url_2020 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/6ff6a6fd-3141-4440-a880-6f60a37fe789/download/script_105774672_20210108153400_combine.csv"
        url_2019 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/ea2e4696-4a2d-429c-9807-d02eb92e0222/download/311_service_requests_2019.csv"
        url_2018 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/2be28d90-3a90-4af1-a3f6-f28c1e25880a/download/311_service_requests_2018.csv"
        url_2017 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/30022137-709d-465e-baae-ca155b51927d/download/311_service_requests_2017.csv"
        url_2016 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/b7ea6b1b-3ca4-4c5b-9713-6dc1db52379a/download/311_service_requests_2016.csv"
        url_2015 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/c9509ab4-6f6d-4b97-979a-0cf2a10c922b/download/311_service_requests_2015.csv"
        url_2014 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/bdae89c8-d4ce-40e9-a6e1-a5203953a2e0/download/311_service_requests_2014.csv"
        url_2013 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/407c5cd0-f764-4a41-adf8-054ff535049e/download/311_service_requests_2013.csv"
        url_2012 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/382e10d9-1864-40ba-bef6-4eea3c75463c/download/311_service_requests_2012.csv"
        url_2011 = "https://data.boston.gov/dataset/8048697b-ad64-4bfc-b090-ee00169f2323/resource/94b499d9-712a-4d2a-b790-7ceec5c9c4b1/download/311_service_requests_2011.csv"


        # Get a list of all CSV files in the directory

        print("Checking files_dict")
        if self.files_dict is None :
            print("files_dict is None")
            try :
                print("trying to call get311URLs")
                files_dict = self.get311URLs()
                print("files_dict is", files_dict)
            except :
                files_dict = {
                '2023': url_2023,
                '2022': url_2022,
                '2021': url_2021,
                '2020': url_2020,
                '2019': url_2019,
                '2018': url_2018,
                '2017': url_2017,
                '2016': url_2016,
                '2015': url_2015,
                '2014': url_2014,
                '2013': url_2013,
                '2012': url_2012,
                '2011': url_2011
                }
        else :
            files_dict = self.files_dict

        all_files = []
        if args != [] :
            for value in args[0] :
                if str(value) in files_dict.keys() :
                    all_files.append(files_dict[str(value)])
        else :
            all_files = files_dict.values 



            

        # Create an empty list to store the dataframes
        dfs = []

        # Loop through the files and load them into dataframes
        for file in all_files:
            df = pd.read_csv(file)
            dfs.append(df)

        #check that the files all have the same number of columns, and the same names
        same_list_num_col = []
        diff_list_num_col = []
        same_list_order_col = []
        diff_list_order_col = []

        for i in range(len(dfs)):

            if dfs[i].shape[1] != dfs[0].shape[1]:
                #print('Error: File', i, 'does not have the same number of columns as File 0')
                diff_list_num_col.append(i)
            else:
                #print('File', i, 'has same number of columns as File 0')
                same_list_num_col.append(i)
            if not dfs[i].columns.equals(dfs[0].columns):
                #print('Error: File', i, 'does not have the same column names and order as File 0')
                diff_list_order_col.append(i)
            else:
                #print('File', i, 'has the same column name and order as File 0')
                same_list_order_col.append(i)

        #print("Files with different number of columns from File 0: ", diff_list_num_col)
        #print("Files with same number of columns as File 0: ", same_list_num_col)
        #print("Files with different column order from File 0: ", diff_list_order_col)
        #print("Files with same column order as File 0: ", same_list_order_col)

        # Concatenate the dataframes into a single dataframe
        df_all = pd.concat(dfs, ignore_index=True)

        return df_all        
        

    def get311URLs(self) :
        print("trying to get csv URLs")

        # specify the URL of the page
        url = "https://data.boston.gov/dataset/311-service-requests"

        # send a GET request to the URL
        response = requests.get(url)

        # parse the HTML content of the page with BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Get the current date and time
        now = datetime.now()

        # Extract the year and print it
        current_year = now.year

        URL_dict = {}

        # find all the anchor tags in the HTML
        # and print out the href attribute, which is the URL
        for link in soup.find_all('a'):
            url = link.get('href')
            if url.endswith('.csv'):
                print("Found URL:", url, "for year", current_year)
                URL_dict[str(current_year)] = url
                current_year = current_year - 1 
        return URL_dict
    
        # Function 1: Generate bin_edges using a fixed hour interval
    def generate_time_bins_fixed_interval(self, hour_interval, max_days):
        max_hours = max_days * 24
        # bin_edges = [0] + [1.3 ** i for i in range(1, int(math.log(max_hours, 1.5)) + 1)]
        bin_edges = [i for i in range(0, max_hours + 1, hour_interval)]
        # bin_edges_days = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,21,28,35,42,49,56,63,70,80,90,100,120,160,180,365,712]
        # for i in bin_edges_days :
        #     bin_edges.append(i*24)
        bin_edges.append(1000000)
        return bin_edges

    # Function 2: Generate bin_edges using statistics
    def generate_time_bins_statistics(self, df, num_intervals=60):
        # Sort DataFrame by survival_time_hours
        df = df.sort_values(by='survival_time_hours')
        # Calculate the size for each bin
        bin_size = len(df) // num_intervals
        # Get bin edges
        bin_edges = [df['survival_time_hours'].iloc[i * bin_size] for i in range(num_intervals)]
        bin_edges.append(df['survival_time_hours'].max())  # add the maximum value
        bin_edges = [0] + bin_edges  # add 0 at the beginning
        return bin_edges
    
    def time_format(self, hours):
        if hours == 0:
            return "0"
        seconds = hours * 3600
        minutes, seconds = divmod(seconds, 60)
        hours, minutes = divmod(minutes, 60)
        days, hours = divmod(hours, 24)
        weeks, days = divmod(days, 7)
        years, weeks = divmod(weeks, 52)

        components = [("y", years), ("w", weeks), ("d", days), ("h", hours), ("m", minutes), ("s", seconds)]
        label = "".join([f"{value}{unit}" for unit, value in components if value > 0])
        return label

    def generate_bin_labels(self, bin_edges, overflow_label=None):
        bin_labels = []
        for i in range(len(bin_edges) - 1):
            start_label = self.time_format(bin_edges[i])
            end_label = self.time_format(bin_edges[i + 1])
            if start_label != end_label:
                label = f"{start_label}-{end_label}"
            else:
                label = start_label
            bin_labels.append(label)

        if overflow_label is not None:
            bin_labels[-1] = overflow_label

        return bin_labels./Boston311SurvDecTree.py
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from datetime import datetime
import pandas as pd
import pickle 
from .Boston311Model import Boston311Model

class Boston311SurvDecTree(Boston311Model):


    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def save(self, filepath, model_file, properties_file):
                
        with open(filepath + '/' + model_file + '.pkl', 'wb') as f:
            pickle.dump(self.model, f)
       
        # Save other properties
        super().save_properties(filepath, properties_file)

    def load(self, json_file, model_file):

        # Load other properties
        super().load_properties(json_file)
        
        with open(model_file, 'rb') as f:
            self.model = pickle.load(f)

    def load_data(self, data=None, train_or_predict='train') :
        return super().load_data(data, train_or_predict)
    
    def enhance_data(self, data, train_or_predict='train'):
        return super().enhance_data(data, train_or_predict)
    
    def apply_scenario(self, data):
        return super().apply_scenario(data)
    
    def clean_data(self, data):
        return super().clean_data(data)
    
    def clean_data_for_prediction(self, data):
        return super().clean_data_for_prediction(data)
    
    def one_hot_encode_with_feature_dict(self, data):
        return super().one_hot_encode_with_feature_dict(data)
    
    def predict( self, data=None ) :
        if data is None :
            data = self.load_data( train_or_predict='predict' )
        else :
            data = self.load_data( data=data, train_or_predict='predict' )
        data = self.enhance_data( data, 'predict')
        clean_data = self.clean_data_for_prediction( data )

        X_predict, y_predict = self.split_data( clean_data )
        y_predict = self.model.predict(X_predict)
        data.insert(0, 'survival_prediction', y_predict)

        return data
    
    def split_data(self, data) :

        X = data.drop(['survival_time_hours', 'event'], axis=1)
        if self.bin_edges is None :
            bin_edges = [0, 24, 168, 672, 8736, 1314870]
            bin_labels = ["0-24 hours", "1-7 days","2-4 weeks","1-12 months","over a year"]
        else :
            if self.bin_labels is None :
                self.bin_edges = self.generate_bin_labels(self.bin_edges)
            bin_edges = self.bin_edges
            bin_labels = self.bin_labels

        y = pd.cut(data['survival_time_hours'], bins=bin_edges, labels=bin_labels)
            
        
        return X, y 
        
    def train_model( self, X, y=[] ) :
        test_accuracy = 0
        self.model, test_accuracy = self.train_tree_model( X, y )
        return test_accuracy

    def train_tree_model ( self, tree_X, tree_y ) :
        start_time = datetime.now()
        print("Starting Training at {}".format(start_time))

        # Split into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(tree_X, tree_y, test_size=0.2, random_state=42)

        # Initialize the model
        model = DecisionTreeClassifier(random_state=42)

        # Fit the model
        model.fit(X_train, y_train)

        y_test_pred = model.predict(X_test)

        # Calculate the accuracy on the testing set
        test_accuracy = accuracy_score(y_test, y_test_pred)
        print('Testing accuracy:', test_accuracy)

        end_time = datetime.now()
        total_time = (end_time - start_time)
        print("Ending Training at {}".format(end_time))
        print("Training took {}".format(total_time))

        return model, test_accuracy
    
    def run_pipeline( self, data=None) :
        if data is None :
            data = self.load_data()
        else :
            data = self.load_data( data=data )
        data = self.enhance_data(data)
        data = self.apply_scenario(data)
        data = self.clean_data(data)
        X, y = self.split_data(data)
        test_accuracy = self.train_model( X, y )
        return test_accuracy./__init__.py
from .Boston311Model import Boston311Model
from .Boston311EventDecTree import Boston311EventDecTree
from .Boston311SurvDecTree import Boston311SurvDecTree
from .Boston311LogReg import Boston311LogReg
from .Boston311LinReg import Boston311LinReg
from .Boston311KerasNN import Boston311KerasNN./unit_tests.py
import numpy as np
import pandas as pd
from pandas.testing import assert_frame_equal, assert_series_equal
from .Boston311Model import Boston311LogReg
from .Boston311Model import Boston311LinReg

def test_data_clean_functions() :
    #set up the test data
    test_data_2022 = pd.DataFrame({'case_enquiry_id': [101004125189,
                        101004161747,
                        101004149944,
                        101004113302,
                        101004122704,
                        101004122479,
                        101004113310,
                        101004113311,
                        101004113328,
                        101004113550],
    'case_status': ['Open',
                    'Closed',
                    'Open',
                    'Closed',
                    'Open',
                    'Open',
                    'Closed',
                    'Closed',
                    'Open',
                    'Closed'],
    'case_title': ['Illegal Rooming House',
                'PublicWorks: Complaint',
                'Space Savers',
                'Parking Enforcement',
                'DISPATCHED Heat - Excessive  Insufficient',
                'Generic Noise Disturbance',
                'Parking Enforcement',
                'General Lighting Request',
                'Loud Parties/Music/People',
                'Requests for Street Cleaning'],
    'city_council_district': ['4', ' ', '4', '2', '7', '8', '3', '6', '1', '2'],
    'closed_dt': [np.nan,
                '2021-02-02 11:45:47',
                np.nan,
                '2022-01-03 00:13:17',
                np.nan,
                np.nan,
                '2022-01-03 00:13:02',
                '2022-04-02 13:01:14',
                np.nan,
                '2022-05-03 05:59:20'],
    'closedphoto': [np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    np.nan,
                    'https://spot-boston-res.cloudinary.com/image/upload/v1641207557/boston/production/o0vkrv9zckukp8httr7g.jpg'],
    'closure_reason': [' ',
                    'Case Closed Case Noted    ',
                    ' ',
                    'Case Closed. Closed date : 2022-01-03 00:13:17.393 Case '
                    'Resolved CLEAR ',
                    ' ',
                    ' ',
                    'Case Closed. Closed date : 2022-01-03 00:13:02.72 Case '
                    'Resolved CLEAR ',
                    'Case Closed. Closed date : Sat Apr 02 13:01:14 EDT 2022 '
                    'Noted ',
                    ' ',
                    'Case Closed. Closed date : Mon Jan 03 05:59:20 EST 2022 '
                    'Noted 3 bags of trash collected at intersection of '
                    'Dartmouth and Warren at 5:56 a.m. on Monday 1/3/22. We '
                    'will return on next scheduled trash day. '],
    'department': ['ISD',
                'PWDx',
                'PWDx',
                'BTDT',
                'ISD',
                'INFO',
                'BTDT',
                'PWDx',
                'INFO',
                'PWDx'],
    'fire_district': ['8', ' ', '8', '6', '9', '3', '8', '9', '3', '4'],
    'latitude': [42.2896,
                42.3594,
                42.2876,
                42.3594,
                42.311,
                42.3657,
                42.291,
                42.3594,
                42.3669,
                42.3594],
    'location': ['27 Lithgow St  Dorchester  MA  02124',
                ' ',
                '492 Harvard St  Dorchester  MA  02124',
                'INTERSECTION of Seaport Blvd & Sleeper St  Boston  MA  ',
                '15 Crawford St  Dorchester  MA  02121',
                '50-150 Causeway St  Boston  MA  02114',
                '16 Frost Ave  Dorchester  MA  02122',
                'INTERSECTION of Boylston St & Moraine St  Jamaica Plain  MA  ',
                '194 Salem St  Boston  MA  02113',
                'INTERSECTION of Warren Ave & Dartmouth St  Boston  MA  '],
    'location_street_name': ['27 Lithgow St',
                            np.nan,
                            '492 Harvard St',
                            'INTERSECTION Seaport Blvd & Sleeper St',
                            '15 Crawford St',
                            '50-150 Causeway St',
                            '16 Frost Ave',
                            'INTERSECTION Boylston St & Moraine St',
                            '194 Salem St',
                            'INTERSECTION Warren Ave & Dartmouth St'],
    'location_zipcode': [2124.0,
                        np.nan,
                        2124.0,
                        np.nan,
                        2121.0,
                        2114.0,
                        2122.0,
                        np.nan,
                        2113.0,
                        np.nan],
    'longitude': [-71.0701,
                -71.0587,
                -71.0936,
                -71.0587,
                -71.0841,
                -71.0617,
                -71.0503,
                -71.0587,
                -71.0546,
                -71.0587],
    'neighborhood': ['Dorchester',
                    ' ',
                    'Greater Mattapan',
                    'South Boston / South Boston Waterfront',
                    'Roxbury',
                    'Boston',
                    'Dorchester',
                    'Jamaica Plain',
                    'Downtown / Financial District',
                    'South End'],
    'neighborhood_services_district': ['8',
                                    ' ',
                                    '9',
                                    '5',
                                    '13',
                                    '3',
                                    '7',
                                    '11',
                                    '3',
                                    '6'],
    'ontime': ['OVERDUE',
            'ONTIME',
            'ONTIME',
            'ONTIME',
            'OVERDUE',
            'ONTIME',
            'ONTIME',
            'OVERDUE',
            'ONTIME',
            'ONTIME'],
    'open_dt': ['2023-05-09 12:59:00',
                '2022-02-02 11:42:49',
                '2022-01-28 19:36:00',
                '2022-01-01 00:36:24',
                '2022-01-11 09:47:00',
                '2022-01-10 21:49:00',
                '2022-01-01 01:13:52',
                '2022-01-01 01:14:39',
                '2022-01-01 03:08:00',
                '2022-01-01 13:51:00'],
    'police_district': ['C11',
                        ' ',
                        'B3',
                        'C6',
                        'B2',
                        'A1',
                        'C11',
                        'E13',
                        'A1',
                        'D4'],
    'precinct': ['1706',
                ' ',
                '1411',
                '0601',
                '1202',
                ' ',
                '1607',
                '1903',
                '0302',
                '0401'],
    'pwd_district': ['07', ' ', '07', '05', '10B', '1B', '07', '02', '1B', '1C'],
    'queue': ['ISD_Housing (INTERNAL)',
            'PWDx_General Comments',
            'PWDx_Space Saver Removal',
            'BTDT_Parking Enforcement',
            'ISD_Housing (INTERNAL)',
            'INFO01_GenericeFormforOtherServiceRequestTypes',
            'BTDT_Parking Enforcement',
            'PWDx_Street Light_General Lighting Request',
            'INFO01_GenericeFormforOtherServiceRequestTypes',
            'PWDx_Missed Trash\\Recycling\\Yard Waste\\Bulk Item'],
    'reason': ['Building',
            'Employee & General Comments',
            'Sanitation',
            'Enforcement & Abandoned Vehicles',
            'Housing',
            'Generic Noise Disturbance',
            'Enforcement & Abandoned Vehicles',
            'Street Lights',
            'Noise Disturbance',
            'Street Cleaning'],
    'source': ['Constituent Call',
            'Constituent Call',
            'Constituent Call',
            'Citizens Connect App',
            'Constituent Call',
            'Constituent Call',
            'Citizens Connect App',
            'City Worker App',
            'Constituent Call',
            'Citizens Connect App'],
    'subject': ['Inspectional Services',
                "Mayor's 24 Hour Hotline",
                'Public Works Department',
                'Transportation - Traffic Division',
                'Inspectional Services',
                "Mayor's 24 Hour Hotline",
                'Transportation - Traffic Division',
                'Public Works Department',
                'Boston Police Department',
                'Public Works Department'],
    'submittedphoto': [np.nan,
                    np.nan,
                    np.nan,
                    'https://311.boston.gov/media/boston/report/photos/61cfe84b05bbcf180c293ece/photo_20220101_003547.jpg',
                    np.nan,
                    np.nan,
                    'https://311.boston.gov/media/boston/report/photos/61cff11805bbcf180c2944b1/report.jpg',
                    np.nan,
                    np.nan,
                    'https://311.boston.gov/media/boston/report/photos/61d0a2af05bbcf180c2993e3/report.jpg'],
    'target_dt': ['2022-01-20 12:59:39',
                '2022-02-16 11:42:49',
                np.nan,
                '2022-01-04 08:30:00',
                '2022-02-10 09:47:22',
                np.nan,
                '2022-01-04 08:30:00',
                '2022-02-15 01:14:45',
                np.nan,
                '2022-01-04 08:30:00'],
    'type': ['Illegal Rooming House',
            'General Comments For a Program or Policy',
            'Space Savers',
            'Parking Enforcement',
            'Heat - Excessive  Insufficient',
            'Undefined Noise Disturbance',
            'Parking Enforcement',
            'General Lighting Request',
            'Loud Parties/Music/People',
            'Requests for Street Cleaning'],
    'ward': ['Ward 17',
            ' ',
            'Ward 14',
            '6',
            'Ward 12',
            '03',
            'Ward 16',
            '19',
            'Ward 3',
            '4']})

    #define the expected output
    tlogistic_test_X_0 = pd.DataFrame({'department_BTDT': [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
    'department_INFO': [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
    'department_ISD': [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    'department_PWDx': [0, 1, 1, 0, 0, 0, 0, 1, 0, 1],
    'reason_Building': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    'reason_Employee & General Comments': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
    'reason_Enforcement & Abandoned Vehicles': [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
    'reason_Generic Noise Disturbance': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    'reason_Housing': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    'reason_Noise Disturbance': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    'reason_Sanitation': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    'reason_Street Cleaning': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    'reason_Street Lights': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    'source_Citizens Connect App': [0, 0, 0, 1, 0, 0, 1, 0, 0, 1],
    'source_City Worker App': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    'source_Constituent Call': [1, 1, 1, 0, 1, 1, 0, 0, 1, 0],
    'subject_Boston Police Department': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    'subject_Inspectional Services': [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    "subject_Mayor's 24 Hour Hotline": [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],
    'subject_Public Works Department': [0, 0, 1, 0, 0, 0, 0, 1, 0, 1],
    'subject_Transportation - Traffic Division': [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
    'ward_number_12': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    'ward_number_14': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    'ward_number_16': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    'ward_number_17': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    'ward_number_19': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    'ward_number_3': [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
    'ward_number_4': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    'ward_number_6': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]}
    )
    tlogistic_test_X_1 = pd.DataFrame({'department_BTDT': [0, 0, 1, 0, 0, 1, 0, 0, 0],
    'department_INFO': [0, 0, 0, 0, 1, 0, 0, 1, 0],
    'department_ISD': [0, 0, 0, 1, 0, 0, 0, 0, 0],
    'department_PWDx': [1, 1, 0, 0, 0, 0, 1, 0, 1],
    'reason_Employee & General Comments': [1, 0, 0, 0, 0, 0, 0, 0, 0],
    'reason_Enforcement & Abandoned Vehicles': [0, 0, 1, 0, 0, 1, 0, 0, 0],
    'reason_Generic Noise Disturbance': [0, 0, 0, 0, 1, 0, 0, 0, 0],
    'reason_Housing': [0, 0, 0, 1, 0, 0, 0, 0, 0],
    'reason_Noise Disturbance': [0, 0, 0, 0, 0, 0, 0, 1, 0],
    'reason_Sanitation': [0, 1, 0, 0, 0, 0, 0, 0, 0],
    'reason_Street Cleaning': [0, 0, 0, 0, 0, 0, 0, 0, 1],
    'reason_Street Lights': [0, 0, 0, 0, 0, 0, 1, 0, 0],
    'source_Citizens Connect App': [0, 0, 1, 0, 0, 1, 0, 0, 1],
    'source_City Worker App': [0, 0, 0, 0, 0, 0, 1, 0, 0],
    'source_Constituent Call': [1, 1, 0, 1, 1, 0, 0, 1, 0],
    'subject_Boston Police Department': [0, 0, 0, 0, 0, 0, 0, 1, 0],
    'subject_Inspectional Services': [0, 0, 0, 1, 0, 0, 0, 0, 0],
    "subject_Mayor's 24 Hour Hotline": [1, 0, 0, 0, 1, 0, 0, 0, 0],
    'subject_Public Works Department': [0, 1, 0, 0, 0, 0, 1, 0, 1],
    'subject_Transportation - Traffic Division': [0, 0, 1, 0, 0, 1, 0, 0, 0],
    'ward_number_12': [0, 0, 0, 1, 0, 0, 0, 0, 0],
    'ward_number_14': [0, 1, 0, 0, 0, 0, 0, 0, 0],
    'ward_number_16': [0, 0, 0, 0, 0, 1, 0, 0, 0],
    'ward_number_19': [0, 0, 0, 0, 0, 0, 1, 0, 0],
    'ward_number_3': [0, 0, 0, 0, 1, 0, 0, 1, 0],
    'ward_number_4': [0, 0, 0, 0, 0, 0, 0, 0, 1],
    'ward_number_6': [0, 0, 1, 0, 0, 0, 0, 0, 0]}
    )
    tlinear_test_X_0 = pd.DataFrame({'department_BTDT': [0, 1, 1, 0, 0],
    'department_INFO': [0, 0, 0, 0, 0],
    'department_ISD': [0, 0, 0, 0, 0],
    'department_PWDx': [1, 0, 0, 1, 1],
    'reason_Building': [0, 0, 0, 0, 0],
    'reason_Employee & General Comments': [1, 0, 0, 0, 0],
    'reason_Enforcement & Abandoned Vehicles': [0, 1, 1, 0, 0],
    'reason_Generic Noise Disturbance': [0, 0, 0, 0, 0],
    'reason_Housing': [0, 0, 0, 0, 0],
    'reason_Noise Disturbance': [0, 0, 0, 0, 0],
    'reason_Sanitation': [0, 0, 0, 0, 0],
    'reason_Street Cleaning': [0, 0, 0, 0, 1],
    'reason_Street Lights': [0, 0, 0, 1, 0],
    'source_Citizens Connect App': [0, 1, 1, 0, 1],
    'source_City Worker App': [0, 0, 0, 1, 0],
    'source_Constituent Call': [1, 0, 0, 0, 0],
    'subject_Boston Police Department': [0, 0, 0, 0, 0],
    'subject_Inspectional Services': [0, 0, 0, 0, 0],
    "subject_Mayor's 24 Hour Hotline": [1, 0, 0, 0, 0],
    'subject_Public Works Department': [0, 0, 0, 1, 1],
    'subject_Transportation - Traffic Division': [0, 1, 1, 0, 0],
    'ward_number_12': [0, 0, 0, 0, 0],
    'ward_number_14': [0, 0, 0, 0, 0],
    'ward_number_16': [0, 0, 1, 0, 0],
    'ward_number_17': [0, 0, 0, 0, 0],
    'ward_number_19': [0, 0, 0, 1, 0],
    'ward_number_3': [0, 0, 0, 0, 0],
    'ward_number_4': [0, 0, 0, 0, 1],
    'ward_number_6': [0, 1, 0, 0, 0]}
    )
    tlinear_test_X_1 = pd.DataFrame({'department_BTDT': [1, 1],
    'department_INFO': [0, 0],
    'department_ISD': [0, 0],
    'department_PWDx': [0, 0],
    'reason_Building': [0, 0],
    'reason_Employee & General Comments': [0, 0],
    'reason_Enforcement & Abandoned Vehicles': [1, 1],
    'reason_Generic Noise Disturbance': [0, 0],
    'reason_Housing': [0, 0],
    'reason_Noise Disturbance': [0, 0],
    'reason_Sanitation': [0, 0],
    'reason_Street Cleaning': [0, 0],
    'reason_Street Lights': [0, 0],
    'source_Citizens Connect App': [1, 1],
    'source_City Worker App': [0, 0],
    'source_Constituent Call': [0, 0],
    'subject_Boston Police Department': [0, 0],
    'subject_Inspectional Services': [0, 0],
    "subject_Mayor's 24 Hour Hotline": [0, 0],
    'subject_Public Works Department': [0, 0],
    'subject_Transportation - Traffic Division': [1, 1],
    'ward_number_12': [0, 0],
    'ward_number_14': [0, 0],
    'ward_number_16': [0, 1],
    'ward_number_17': [0, 0],
    'ward_number_19': [0, 0],
    'ward_number_3': [0, 0],
    'ward_number_4': [0, 0],
    'ward_number_6': [1, 0]}
    )
    tlinear_test_X_2 = pd.DataFrame({'department_BTDT': [1, 1, 0, 0],
    'department_INFO': [0, 0, 0, 0],
    'department_ISD': [0, 0, 0, 0],
    'department_PWDx': [0, 0, 1, 1],
    'reason_Building': [0, 0, 0, 0],
    'reason_Employee & General Comments': [0, 0, 0, 0],
    'reason_Enforcement & Abandoned Vehicles': [1, 1, 0, 0],
    'reason_Generic Noise Disturbance': [0, 0, 0, 0],
    'reason_Housing': [0, 0, 0, 0],
    'reason_Noise Disturbance': [0, 0, 0, 0],
    'reason_Sanitation': [0, 0, 0, 0],
    'reason_Street Cleaning': [0, 0, 0, 1],
    'reason_Street Lights': [0, 0, 1, 0],
    'source_Citizens Connect App': [1, 1, 0, 1],
    'source_City Worker App': [0, 0, 1, 0],
    'source_Constituent Call': [0, 0, 0, 0],
    'subject_Boston Police Department': [0, 0, 0, 0],
    'subject_Inspectional Services': [0, 0, 0, 0],
    "subject_Mayor's 24 Hour Hotline": [0, 0, 0, 0],
    'subject_Public Works Department': [0, 0, 1, 1],
    'subject_Transportation - Traffic Division': [1, 1, 0, 0],
    'ward_number_12': [0, 0, 0, 0],
    'ward_number_14': [0, 0, 0, 0],
    'ward_number_16': [0, 1, 0, 0],
    'ward_number_17': [0, 0, 0, 0],
    'ward_number_19': [0, 0, 1, 0],
    'ward_number_3': [0, 0, 0, 0],
    'ward_number_4': [0, 0, 0, 1],
    'ward_number_6': [1, 0, 0, 0]}
    )
    tlogistic_test_y_0 = pd.Series({0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 0, 6: 1, 7: 1, 8: 0, 9: 1}
    )
    tlogistic_test_y_1 = pd.Series({1: 1, 2: 0, 3: 1, 4: 0, 5: 0, 6: 1, 7: 0, 8: 0, 9: 0}
    )
    tlinear_test_y_0 = pd.Series({1: -8759.950555555555,
    3: 47.61472222222222,
    6: 46.986111111111114,
    7: 2195.776388888889,
    9: 2920.1388888888887}
    )
    tlinear_test_y_1 = pd.Series({3: 47.61472222222222, 6: 46.986111111111114}
    )
    tlinear_test_y_2 = pd.Series({3: 47.61472222222222,
    6: 46.986111111111114,
    7: 2195.776388888889,
    9: 2920.1388888888887}
    )

    #create Boston311Model objects
    


    #call the function with the test data
    '''
    logistic_test_X_0, logistic_test_y_0 = clean_and_split_for_logistic(test_data_2022, [0])
    logistic_test_X_1, logistic_test_y_1 = clean_and_split_for_logistic(test_data_2022, [1, 2])

    linear_test_X_0, linear_test_y_0 = clean_and_split_for_linear(test_data_2022, [0])
    linear_test_X_1, linear_test_y_1 = clean_and_split_for_linear(test_data_2022, [1, 2])
    linear_test_X_2, linear_test_y_2 = clean_and_split_for_linear(test_data_2022, [2])
    '''

    myBoston311Model_0 = Boston311LogReg(feature_columns=['subject', 'reason', 'department', 'source', 'ward_number' ],
                                      scenario={})
    test_data_enhanced_0 = myBoston311Model_0.enhance_data(test_data_2022)
    test_data_cleaned_0 = myBoston311Model_0.clean_data(test_data_enhanced_0)
    logistic_test_X_0, logistic_test_y_0 = myBoston311Model_0.split_data(test_data_cleaned_0)

    myBoston311Model_1 = Boston311LogReg(feature_columns=['subject', 'reason', 'department', 'source', 'ward_number' ],
                                      scenario={'dropOpen':'2023-04-09',
                                      'eventToZeroforSurvivalTimeGreaterThan': 2678400})
    test_data_enhanced_1 = myBoston311Model_1.enhance_data(test_data_2022)
    test_data_cleaned_1 = myBoston311Model_1.clean_data(test_data_enhanced_1)
    logistic_test_X_1, logistic_test_y_1 = myBoston311Model_1.split_data(test_data_cleaned_1)

    myBoston311Model_lin0 = Boston311LinReg(feature_columns=['subject', 'reason', 'department', 'source', 'ward_number' ],
                                      scenario={'dropOpen':'2021-12-31'})
    test_data_enhanced_lin0 = myBoston311Model_lin0.enhance_data(test_data_2022)
    test_data_cleaned_lin0 = myBoston311Model_lin0.clean_data(test_data_enhanced_lin0)
    linear_test_X_0, linear_test_y_0 = myBoston311Model_lin0.split_data(test_data_cleaned_lin0)

    myBoston311Model_lin1 = Boston311LinReg(feature_columns=['subject', 'reason', 'department', 'source', 'ward_number' ],
                                      scenario={'dropOpen':'2021-12-31',
                                                'survivalTimeMax':2678400,
                                                'survivalTimeMin':0})
    test_data_enhanced_lin1 = myBoston311Model_lin1.enhance_data(test_data_2022)
    test_data_cleaned_lin1 = myBoston311Model_lin1.clean_data(test_data_enhanced_lin1)
    linear_test_X_1, linear_test_y_1 = myBoston311Model_lin1.split_data(test_data_cleaned_lin1)


    myBoston311Model_lin2 = Boston311LinReg(feature_columns=['subject', 'reason', 'department', 'source', 'ward_number' ],
                                      scenario={'dropOpen':'2021-12-31',
                                                'survivalTimeMin':0})
    test_data_enhanced_lin2 = myBoston311Model_lin2.enhance_data(test_data_2022)
    test_data_cleaned_lin2 = myBoston311Model_lin2.clean_data(test_data_enhanced_lin2)
    linear_test_X_2, linear_test_y_2 = myBoston311Model_lin2.split_data(test_data_cleaned_lin2)


    #check if the function output matches the expected output when reindexed

    test_data = [
        (logistic_test_X_0, tlogistic_test_X_0),
        (logistic_test_X_1, tlogistic_test_X_1),
        (linear_test_X_0, tlinear_test_X_0),
        (linear_test_X_1, tlinear_test_X_1),
        (linear_test_X_2, tlinear_test_X_2),
        (logistic_test_y_0, tlogistic_test_y_0),
        (logistic_test_y_1, tlogistic_test_y_1),
        (linear_test_y_0, tlinear_test_y_0),
        (linear_test_y_1, tlinear_test_y_1),
        (linear_test_y_2, tlinear_test_y_2)
    ]

    for data, expected in test_data:
        if isinstance(data, pd.DataFrame):
            # Sort the DataFrames by index and column names
            data = data.sort_index(axis=0).sort_index(axis=1)
            expected = expected.sort_index(axis=0).sort_index(axis=1)
            # Reset the index to avoid issues with different index types
            data = data.reset_index(drop=True)
            expected = expected.reset_index(drop=True)
            # Compare the DataFrames and assert that they are equal
            #print("Dataframe indices:")
            #print(data.index)
            #print(expected.index)
            #print("Dataframe columns:")
            #print(data.columns)
            #print(expected.columns)
            #diff = data.compare(expected)
            #if not diff.empty:
            #    print(f"DataFrames are different:\n{diff}")
            assert_frame_equal(data, expected, check_dtype=False)
        elif isinstance(data, pd.Series):
            # Sort the Series by index
            #data = data.sort_index(axis=0)
            data = data.rename(None)
            #expected = expected.sort_index(axis=0)
            # Compare the Series and assert that they are equal
            #print("Series indices:")
            #print(data.index)
            #print(expected.index)
            #diff = data.compare(expected)
            #if not diff.empty:
            #    print(f"Series are different:\n{diff}")
            assert_series_equal(data, expected, check_dtype=False)

